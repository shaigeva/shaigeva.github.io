<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>AI Feedback Loop, With an Example | Shai Geva</title>
<meta name="keywords" content="">
<meta name="description" content="(this post is part of a series about creating
production-grade maintainable AI-first projects, using AI-first design patterns and frameworks)
In the previous post we mentioned that an internal AI feedback loop will be central to all our design patterns.
In this post, we&rsquo;ll talk a bit about why, and give a simple example for what it might look like, to get us going.
A reality of imperfection
A fundamental truth about creating code (and doing things in general) is that sometimes it&rsquo;s not perfect.">
<meta name="author" content="Shai Geva">
<link rel="canonical" href="https://shaigeva.com/posts/ai_frameworks/02_ai_feedback_loop_example/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shaigeva.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shaigeva.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shaigeva.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shaigeva.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://shaigeva.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://shaigeva.com/posts/ai_frameworks/02_ai_feedback_loop_example/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-YWC9X8YE9C"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-YWC9X8YE9C');
        }
      </script><meta property="og:url" content="https://shaigeva.com/posts/ai_frameworks/02_ai_feedback_loop_example/">
  <meta property="og:site_name" content="Shai Geva">
  <meta property="og:title" content="AI Feedback Loop, With an Example">
  <meta property="og:description" content="(this post is part of a series about creating production-grade maintainable AI-first projects, using AI-first design patterns and frameworks)
In the previous post we mentioned that an internal AI feedback loop will be central to all our design patterns.
In this post, we’ll talk a bit about why, and give a simple example for what it might look like, to get us going.
A reality of imperfection A fundamental truth about creating code (and doing things in general) is that sometimes it’s not perfect.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-03T13:01:56+03:00">
    <meta property="article:modified_time" content="2025-07-03T13:01:56+03:00">
    <meta property="og:image" content="https://shaigeva.com/ai_frameworks/ai_feedback_loop_ascii_art.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://shaigeva.com/ai_frameworks/ai_feedback_loop_ascii_art.png">
<meta name="twitter:title" content="AI Feedback Loop, With an Example">
<meta name="twitter:description" content="(this post is part of a series about creating
production-grade maintainable AI-first projects, using AI-first design patterns and frameworks)
In the previous post we mentioned that an internal AI feedback loop will be central to all our design patterns.
In this post, we&rsquo;ll talk a bit about why, and give a simple example for what it might look like, to get us going.
A reality of imperfection
A fundamental truth about creating code (and doing things in general) is that sometimes it&rsquo;s not perfect.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://shaigeva.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "AI Feedback Loop, With an Example",
      "item": "https://shaigeva.com/posts/ai_frameworks/02_ai_feedback_loop_example/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "AI Feedback Loop, With an Example",
  "name": "AI Feedback Loop, With an Example",
  "description": "(this post is part of a series about creating production-grade maintainable AI-first projects, using AI-first design patterns and frameworks)\nIn the previous post we mentioned that an internal AI feedback loop will be central to all our design patterns.\nIn this post, we\u0026rsquo;ll talk a bit about why, and give a simple example for what it might look like, to get us going.\nA reality of imperfection A fundamental truth about creating code (and doing things in general) is that sometimes it\u0026rsquo;s not perfect.\n",
  "keywords": [
    
  ],
  "articleBody": "(this post is part of a series about creating production-grade maintainable AI-first projects, using AI-first design patterns and frameworks)\nIn the previous post we mentioned that an internal AI feedback loop will be central to all our design patterns.\nIn this post, we’ll talk a bit about why, and give a simple example for what it might look like, to get us going.\nA reality of imperfection A fundamental truth about creating code (and doing things in general) is that sometimes it’s not perfect.\nIt can be unclear specs, incorrect implicit assumptions or just mistakes.\nEither way - when a change to the code is made (either by a human or by an AI agent), there’s some chance that it won’t be what we need.\nLet’s call this chance “bug rate”.\nHumans have a non-zero bug rate, and so does AI. What we want, is to improve productivity given that fact.\nThe feedback loop As we all know (it’s our day to day, after all), the way we generally deal with this reality (regardless of AI) is by iterating.\nWe plan - decide what to do next. Then we do the thing. We verify - get feedback (check if that thing is a step in the right direction, so we can course-correct if not). Then we repeat until done.\nWhen we use AI agents, we incorporate them into our feedback loop - they can help plan / do or verify.\nIf the agent has a high bug rate (==its output has a lot of mistakes), we humans have more work to do.\nSo we want to find a way to reduce the AI agent bug rate, given that LLMs have a non-zero bug rate.\nThe way to do that, is to give agents their own internal feedback loops.\nWe want to give the agent ways to do some verifications on their output and auto-fix bugs before returning their output to us.\nWe don’t expect bug rate to go to zero (== agent finds ALL mistakes) - but maybe it’s possible to find MOST mistakes.\nThis idea is not news (agents today already do some verifications - they’ll use a linter if available, run tests in some cases).\nWhat I’m saying is that we want to make this one of the “stars of the show”.\nSmaller is better An important consideration, is that as a rule of thumb, smaller “things” tend to work better here.\nChanges in small code bases have lower bug rates than changes in large code bases. Small changes have lower bug rates than large changes. A feedback loop with wany small iterations with some verification after each, works much better than few very large iterations (you wouldn’t write code for 5 days without ever running it or even getting IDE warnings, right?). Intuitively, we should be aiming for the same thing with AI - small, contained changes with good verification after each iteration.\nA feedback loop example When I’m able to set up an effective internal AI feedback loop (which is not often unfortunately) - that’s when I’m able to get the most of AI agents.\nNow, although this might be trivial to some devs that have dived deep into AI-assisted coding, I feel that for many people, the concept of an “internal AI feedback loop that auto-heals errors” is pretty abstract. So - let’s look at an example.\nIt’s a small project (python API service) that’s set up for a feedback loop (I went with Cursor, since it’s the most common ATM).\nWhat we want to see here is the feedback loop “in action” - we give a task to the AI agent, and we want to see how it validates and auto-corrects itself before proceeding.\nA note about complexity of the project: I think of it as simple, but not trivial. This is a small project with fairly simple (and un-optimized) Cursor setup.\nExisting vibe coding tools already one-off projects which are far more impressive.\nThe objective here is not to show something that’s difficult for existing tools, but rather to show what a feedback loop when changing existing code might look like.\nLater posts will have projects which are more complex both logically and technically.\nWhat’s the project? I initially wanted to go with a super-trivial example like a simple todo list, but with current AI models, I was concerned they just won’t make enough mistakes with something so simple, and without mistakes you can’t show a feedback loop.\nSo I went with a “bigger todo list” - a backend API for a task management service, where we have tasks and projects with filtering etc.\nI also created a frontend so it’s easier to play with the backend, but we’ll focus on the backend for now.\nThe project is set up with technical layers and test suites that are enough to make this size of project fairly stable.\nThe AI agent has configuration (Cursor rules) that allow it to maintain these: it knows the archituctural layers, it knows which tests suites are the most important, it understands it must run tests and other validations like linting and type checking after each code change.\nThere is also a “taskmaster” setup so the agent can break down tasks into smaller tasks and work on them one by one, which restricts task size (and hence makes us have less blundering).\nI’m providing some details about the service here. These are useful\nThe repository can be found at:\nA few details about the service:\nDomain:\nWe have projects. Project contain tasks (each task always has a single project associated with it). Tasks can be completed or deleted. Projects can be archived or deleted (if they are empty). Tasks can be filtered. Technical: This is a rough diagram of the application layers and the layers that the central test suites target:\n(if you’re not a Python person - no problem. Understanding all the details is not the main thing here. Read through, you’ll get the gist):\nPython FastAPI framework DB is locally running SQLite (this is only a demo project) No auth (again, just a simple demo) Code design: There are fairly standard app layers: Top-level application object API handlers DAL (data access layer) Interfaces: HTTP API structure is defined using data structures (pydantic data models). The interface to the DAL also works only with data structures (no ORM / SQL happens outside the DAL). Test design: The DAL has tests: /tests/dal. Mostly /tests/dal/repository.py. These tests only use the DAL interface, meaning they only create and access data through the DAL, they don’t directly touch the DB. There are service-level tests: /tests/api. They test the HTTP API using a “TestClient”, which is a utility provided by the web framework we’re using to allow us to simulate http requests to the service - so we “send http requests” to a “running server” inside a test process, but there’s no actual server running and the requsets are really just function calls. This is a fairly standard utility in many backend frameworks. These tests only operate at the HTTP API level - data is created by calling HTTP endpoints, for example, it’s not directly inserted into the DB. There are a bunch of other tests, but they’re less interesting ATM and the feedback loop will actually be just fine without most of them. Here’s a rough\nHere’s a short video (no sound) to show what\nThe idea is not to discuss any specific technique,\nI’m giving an example of a project that’s very small,\nIn this post I’ll focus on the reason I think frameworks are the direction and what I think they will include, where following posts will dive deeper into the technical details.\nThe objective here is not to “convince” (that would not fit in a blog post :) ), but to lay out the main points.\nAt a very high level, the rationale is this:\nIt makes sense to “aim our coding for the AI”, because it’ll be the new norm. With the current way we’re making software, it’s just not realistic to make a paradigm shift happen, because AI cannot have a feedback loop that’s good enough to change code at an acceptable speed and self-heal enough issues. Therefore, I believe the best forward is to have frameworks that dictate some important aspects of the code and workflow, in order to give AI what it needs so it can do its job. Let’s dig in.\nAn industrial revolution Soon enough, the vast majority of code will be written by AI.\nSo in a sense, this is a large automation movement.\nThings human craftsmen currently make will be handed over en masse to the machine.\nGiven that, it makes sense to think about the common case for software creation, which will soon be the automated process.\nAnd as with other automations, the automation might involve a redesign.\nThe printing press is not a faster pen. An assembly line is not a faster human craftsman. They create in a different way, and their outcome is a little different.\nAnd we should expect the same from automating software creation.\nThe size of an iteration The shorter a feedback loop is, the better.\nAny mistake can send you off in the wrong direction, so you want to get feedback as early as possible.\nA simple example that can help visualize why this is important would be to imagine writing code for an entire week without ever running it, compiling it or getting warnings from the IDE.\nWe all know that the result is going to be pretty nasty. No serious developer will choose to work like this.\nThis matters A LOT (personally, “iteration size” is the main “metric” I look at for improving productivity).\nAI and our feedback loops Although not always discussed this way, a lot of what we’re doing now with AI tools is integrating them into our own feedback loop.\nAnd when you look at it from this angle, you can see that a lot of the techniques we use to improve the performance of human+ai are just optimizing some part of our feedback loop - adding steps in the process for detailed planning, breaking down to smaller tasks, making sure the correct context gets in so it’s easier for the AI to “do” etc.\nAI’s internal feedback loop today An observation I’m making here is that the AI has (or can have) its own feedback loop before it hands the latest change over to us.\nMy own experience, and I’m sure many others’ as well, is that the best outcome I get is if I set up an AI assistant for a feedback loop. Each step looks like this:\nUnderstand what to do next Create something small Verify. For example, write a new test + Run all relevant tests. If something breaks - the next action would be to fix it. I let this run automatically in a loop until either all tests are green, it gives up or it starts going crazy.\nThis is possible, for example with Cursor, for some time now. In fact, Cursor will try to self-heal what it can out of the box - stuff like lint errors.\nThis works extremely well. The AI having its own internal self-healing feedback loop, backed by the right tests to make me feel safe-enough that nothing breaks, is a different category of productivity.\nSome bugs still get through to me. I test, but it’s kind of “skipping to the end of the process” - I would also do the same tests if I wrote the code, and find similar bugs.\nWhen I can set it up, it’s just great. But… for most of my work, I can’t.\nAI’s internal feedback loop tomorrow When I distill the situation as far as I can, this is what remains.\nAn LLM will statistically do the wrong thing sometimes. Large, standard codebases and systems are FAR too complex for these “imperfections” to be rare. So if we want to avoid most of these bugs reaching a human - we must allow the AI to self-correct using a feedback loop. A feedback loop is therefore sort-of a “constant” in this “movement to AI”. A requirement that must be satistified for large, ongoing projects.\nIt follows that at least for complex projects (though I would say for almost all projects), our best bet is to adopt coding practices that work well with such a feedback loop.\nThese kinds of coding practices are what I call AI-first frameworks.\nWhat are some limiting factors? So, why don’t we have such feedback loops for our projects today?\nWhy don’t we just tell the AI “here’s a project with a million lines of code. Add a small feature”, and then it would do changes and verify them itself?\nIf you’re following the trend on social media, you can see that improvements happen all the time, both in tools and how to use them.\nBut if we look from the perspective of the plan-do-verify stages, we can see that almost everything falls between “plan” and “do”.\nThere are definitely improvements in “verify” (and companies that emphasize quality*), but for the “general project” there really is no big news. AI assistants will generate code and if you ask they’ll generate some tests. But there’s no tool that you can throw on your project that would make changes and make sure nothing broke.\nSo\nTwitter is full of tips and tricks on rule files and project documentation, workflows that break down work into small steps etc.\nThe way I see it, we have two significant bottlenecks:\nFirst: understanding complex code well, incl. flow execution. I believe there are codebases where this works pretty well, but certainly not consistently enough.\nAnd second: the real limiting factor of the AI software movement - “verify”.\nThe tests.\nWell, mostly the tests - there are other verifications.\nFor example, automatically taking a one-time screenshot of a UI component “before and after” and comparing is very much a verification, though it’s not considered a “test” by common terminology. But I’ll usually include all verification in “tests” to be a little less verbose.\nThe tests I’m proficient with tests. I have given testing a lot of attention for two decades, I give talks about tests, I had successes and failures and I have an “arsenal” of effective techniques.\nBecause of that, I know it’s not realistic to add good tests to most code bases. At least, good tests that will be fast enough.\nCode is usually not testable if it’s not designed to be testable.\nSome common issues are\nLogical complexity, where some component does a lot of stuff and it’s difficult to isolate what you want to test in a way that’s both reliable and simple enough. Especially for complex flows that include multiple steps and many components with non-trivial state changes. Technical complexity, where it’s difficult to set up things. A simple example would be data samples for complex processes, but the more hairy issues involve things like like relying on some external thing for which it’s tricky to simulate failure modes. Side-effects, where the code interacts with the outside world in some way. Consider something like this: we have an arbitrary microservice architecture with 50 repos, where some services are written in typescript, some are written in python without type hints, and the team has even braved Rust on one of them. We mostly use Postgres with redis, but some workflows use s3. 5 3rd party APIs handle a few concerns like payments, opening support tickets or similar. We might not even have a trivial way to set up “new clean test” on a local machine. Now, the AI made a significant change to a shared python library that deals with the DB.\nWhen you call the typescript web service, which will then call 15 different services, how easy will it be for the us to trust that all read-only transactions stay read-only and that the latest change doesn’t accidentally override a value in some obscure sequence of events where Stripe returns 502 once every 3 weeks? We can’t run all the options. It might not even be possible to technically test all failure modes for all 3rd party services that we use. And would it not run for 30 years if we tried?\nAnd since we’re not testing it out, what will we do? Do you trust any model that tells you “I’ve seen all the context, there’s no sequence of events that triggers a dormant bug from 2 years ago and breaks this”?\nWe need to trust it, or this doesn’t work.\nNow, there are solutions, but they require specific design patterns and tooling.\nI’ll finish this point here, because this is already a very long post.\nIf what I wrote here has not convinced you - let’s mark this as a debt to be explained, and one of the planned posts in the series will take a deep-dive into this point.\nFor now, let’s assume that adding good, fast tests to an arbitrary code base is very hard.\nHard enough that to make the paradigm shift it’s easier to switch coding styles than it is to create tools that can deal with the old code style.\nNOTE: maybe say something like If you want to make sure that an endpoint is read only, you can do ad-hoc static analysis to try to make sure that there is no code path where an sql query that changes data runs, you can write thousands of tests covering many possible scenarios for what might have been in the DB to begin with etc.\nYou can also use some permissions framework and only give that endpoint read-only access to the DB. Then, in order to test that “endpoint_x doesn’t write to the DB” we just need to make sure that we use the permission framework and that the endpoint was indeed assigned read-only permissions.\nIt’s not a very common approach today, but it’s easy to see how it eliminates categories of bugs very easily and very reliably.\nSecurity I’d be remiss if I didn’t mention security.\nEven for something as simple as testing locally - the disk on our machine typically has a .env file with secrets.\nAutomatically running a unit test that was created automatically, on code that was created automatically might leak that.\nSo if we want to be able to just let the AI do its thing with minimal supervision, it might be a good idea to have some guardrails.\nFrameworks It therefore seems that the most make-sense way forward, is to build AI-compatibility into every part of software creation - design, coding, workflows, security etc.\nIf tests on a general codebase are hard, let’s have a design that guarantees our codebases can be tested.\nIf we need to avoid leaking information, we probably need to address that.\nIt’s difficult enough to solve these issues if we “own” the tech stack and design, and can freely run any part of the code that we want.\nIf we don’t, it just becomes exponentially more difficult.\nWe need to understand what AI needs, and give it that, instead of trying to have it “just work” on whatever arbitrary code base we happen to have.\nThis is what I think of as an AI-native framework.\nIt would take a certain use case (hopefully relatively broad) and design some of the of the aspects of the project to make them AI-compatible.\nI don’t know what would be the granularity of these frameworks\nMaybe it’ll be a single idea like testing at a certain layer in an http web server and you would compose a bunch of these together Maybe it’ll be “FastAPI + Postgres + React has at least these layers and tests that look like these” Maybe it’ll be much more generic than that. Maybe the only frameworks we see will be part of paid platforms that also contain the AI engines that use the frameworks. You could satisfy a lot of these requirements by having something like Wix and have custom-generated-code hook into different parts of the system. I don’t think this is where it’s going, but maybe. What I am sure of is that frameworks will set up software projects so that AI engines can have effective feedback loops.\nThey will include at least some strict design and testing guidelines, and will probably include ways to document and declare a spec for parts of the software.\nI do believe the successful ones will probably be technology-specific, at least to some degree.\nThere are things we can do with some technologies that we can’t do with others, and they matter.\nFor example, we don’t have static types in javascript (put aside jsdoc for a sec), but we do have them in typescript. This changes some tradeoffs of what’s easy for the AI to self-heal and what is not.\nI believe some of the conventions we see in these frameworks will be very similar to what some teams already do.\nBut some will not. It is easier for a machine to do some things that are difficult for us and vice versa.\nFor example, the way it makes sense for an AI to use types is typescript is not the same in my opinion as the common conventios. I’ll dig into this quite a bit later on.\nSome tooling will almost certainly be very different from today.\nI don’t think we’ll have Python code generation at scale without at least something like a sandbox be part of common frameworks. Maybe even the ability to easily configure file access, limitations on network calling etc.\nWrapping up I hope I managed to make the case that it’ll very difficult to have industrial-strength code generation at scale for arbitrary code bases.\nAnd that we must therefore explore frameworks that would allow us to control enough of the structure of the code that would allow the AI to have an effective internal feedback loop.\nIn the next post, we’ll start exploring an example of what a simple framework might look like.\nAbout companies that emphasize quality - I’m biased, as I worked there in the past, but if you’re interested you might want to have a look at Qodo. \u003c\u003c previous post: AI-First Development Frameworks (intro) | next post: (coming soon) \u003e\u003e ",
  "wordCount" : "3725",
  "inLanguage": "en",
  "image":"https://shaigeva.com/ai_frameworks/ai_feedback_loop_ascii_art.png","datePublished": "2025-07-03T13:01:56+03:00",
  "dateModified": "2025-07-03T13:01:56+03:00",
  "author":{
    "@type": "Person",
    "name": "Shai Geva"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shaigeva.com/posts/ai_frameworks/02_ai_feedback_loop_example/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shai Geva",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shaigeva.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shaigeva.com/" accesskey="h" title="Shai Geva (Alt + H)">Shai Geva</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shaigeva.com/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://shaigeva.com/talks" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="https://shaigeva.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      AI Feedback Loop, With an Example
    </h1>
    <div class="post-meta"><span title='2025-07-03 13:01:56 +0300 IDT'>July 3, 2025</span>&nbsp;·&nbsp;<span>Shai Geva</span>

</div>
  </header> 
<figure class="entry-cover">
        <img loading="eager" src="https://shaigeva.com/ai_frameworks/ai_feedback_loop_ascii_art.png" alt="">
        
</figure>
  <div class="post-content"><p><em>(this post is part of a <a href="https://shaigeva.com/posts/ai_frameworks/01_ai_frameworks_intro/">series</a> about creating
production-grade maintainable AI-first projects, using AI-first design patterns and frameworks)</em></p>
<p>In the previous post we mentioned that an internal AI feedback loop will be central to all our design patterns.<br>
In this post, we&rsquo;ll talk a bit about why, and give a simple example for what it might look like, to get us going.</p>
<h2 id="a-reality-of-imperfection">A reality of imperfection<a hidden class="anchor" aria-hidden="true" href="#a-reality-of-imperfection">#</a></h2>
<p>A fundamental truth about creating code (and doing things in general) is that sometimes it&rsquo;s not perfect.</p>
<p>It can be unclear specs, incorrect implicit assumptions or just mistakes.<br>
Either way - when a change to the code is made (either by a human or by an AI agent),
there&rsquo;s some chance that it won&rsquo;t be what we need.<br>
Let&rsquo;s call this chance &ldquo;bug rate&rdquo;.</p>
<p>Humans have a non-zero bug rate, and so does AI.
What we want, is to improve productivity given that fact.</p>
<h2 id="the-feedback-loop">The feedback loop<a hidden class="anchor" aria-hidden="true" href="#the-feedback-loop">#</a></h2>
<p>As we all know (it&rsquo;s our day to day, after all), the way we generally deal with this reality (regardless of AI) is by
iterating.</p>
<ol>
<li>We plan - decide what to do next.</li>
<li>Then we do the thing.</li>
<li>We verify - get feedback (check if that thing is a step in the right direction, so we can course-correct if not).</li>
</ol>
<p>Then we repeat until done.</p>
<p>When we use AI agents, we incorporate them into our feedback loop - they can help plan / do or verify.</p>
<p>If the agent has a high bug rate (==its output has a lot of mistakes), we humans have more work to do.<br>
So we want to find a way to reduce the AI agent bug rate, given that LLMs have a non-zero bug rate.</p>
<p>The way to do that, is to give agents their own internal feedback loops.<br>
We want to give the agent ways to do some verifications on their output and auto-fix bugs before returning their output
to us.<br>
We don&rsquo;t expect bug rate to go to zero (== agent finds ALL mistakes) - but maybe it&rsquo;s possible to find MOST mistakes.</p>
<p>This idea is not news (agents today already do some verifications - they&rsquo;ll use a linter if available, run tests in
some cases).<br>
What I&rsquo;m saying is that we want to make this one of the &ldquo;stars of the show&rdquo;.</p>
<h2 id="smaller-is-better">Smaller is better<a hidden class="anchor" aria-hidden="true" href="#smaller-is-better">#</a></h2>
<p>An important consideration, is that as a rule of thumb, smaller &ldquo;things&rdquo; tend to work better here.</p>
<ul>
<li>Changes in small code bases have lower bug rates than changes in large code bases.</li>
<li>Small changes have lower bug rates than large changes.</li>
<li>A feedback loop with wany small iterations with some verification after each, works much better than few very large
iterations (you wouldn&rsquo;t write code for 5 days without ever running it or even getting IDE warnings, right?).</li>
</ul>
<p>Intuitively, we should be aiming for the same thing with AI - small, contained changes with good verification after
each iteration.</p>
<h2 id="a-feedback-loop-example">A feedback loop example<a hidden class="anchor" aria-hidden="true" href="#a-feedback-loop-example">#</a></h2>
<p>When I&rsquo;m able to set up an effective internal AI feedback loop (which is not often unfortunately) - that&rsquo;s when I&rsquo;m
able to get the most of AI agents.</p>
<p>Now, although this might be trivial to some devs that have dived deep into AI-assisted coding, I feel that for many
people, the concept of an &ldquo;internal AI feedback loop that auto-heals errors&rdquo; is pretty abstract.
So - let&rsquo;s look at an example.<br>
It&rsquo;s a small project (python API service) that&rsquo;s set up for a feedback loop (I went with Cursor, since it&rsquo;s the most
common ATM).</p>
<p>What we want to see here is the feedback loop &ldquo;in action&rdquo; - we give a task to the AI agent, and we want to see how it
validates and auto-corrects itself before proceeding.</p>
<h3 id="a-note-about-complexity-of-the-project">A note about complexity of the project:<a hidden class="anchor" aria-hidden="true" href="#a-note-about-complexity-of-the-project">#</a></h3>
<p>I think of it as simple, but not trivial.
This is a small project with fairly simple (and un-optimized) Cursor setup.<br>
Existing vibe coding tools already one-off projects which are far more impressive.<br>
The objective here is not to show something that&rsquo;s difficult for existing tools, but rather to show what a feedback loop
when changing existing code might look like.<br>
Later posts will have projects which are more complex both logically and technically.</p>
<h3 id="whats-the-project">What&rsquo;s the project?<a hidden class="anchor" aria-hidden="true" href="#whats-the-project">#</a></h3>
<p>I initially wanted to go with a super-trivial example like a simple todo list, but with current AI models, I was
concerned they just won&rsquo;t make enough mistakes with something so simple, and without mistakes you can&rsquo;t show a feedback
loop.</p>
<p>So I went with a &ldquo;bigger todo list&rdquo; - a backend API for a task management service, where we have tasks and projects with
filtering etc.<br>
I also created a frontend so it&rsquo;s easier to play with the backend, but we&rsquo;ll focus on the backend for now.</p>
<p>The project is set up with technical layers and test suites that are enough to make this size of project fairly stable.<br>
The AI agent has configuration (Cursor rules) that allow it to maintain these: it knows the archituctural layers, it
knows which tests suites are the most important, it understands it must run tests and other validations like linting
and type checking after each code change.<br>
There is also a &ldquo;taskmaster&rdquo; setup so the agent can break down tasks into smaller tasks and work on them one by one,
which restricts task size (and hence makes us have less blundering).</p>
<p>I&rsquo;m providing some details about the service here. These are useful</p>
<p>The repository can be found at:</p>
<p>A few details about the service:<br>
Domain:</p>
<ul>
<li>We have projects.</li>
<li>Project contain tasks (each task always has a single project associated with it).</li>
<li>Tasks can be completed or deleted.</li>
<li>Projects can be archived or deleted (if they are empty).</li>
<li>Tasks can be filtered.</li>
</ul>
<p>Technical:
This is a rough diagram of the application layers and the layers that the central test suites target:</p>
<p>(if you&rsquo;re not a Python person - no problem. Understanding all the details is not the main thing here. Read
through, you&rsquo;ll get the gist):</p>
<ul>
<li>Python</li>
<li>FastAPI framework</li>
<li>DB is locally running SQLite (this is only a demo project)</li>
<li>No auth (again, just a simple demo)</li>
<li>Code design:
<ul>
<li>There are fairly standard app layers:
<ul>
<li>Top-level application object</li>
<li>API handlers</li>
<li>DAL (data access layer)</li>
</ul>
</li>
<li>Interfaces:
<ul>
<li>HTTP API structure is defined using data structures (pydantic data models).</li>
<li>The interface to the DAL also works only with data structures (no ORM / SQL happens outside the DAL).</li>
</ul>
</li>
</ul>
</li>
<li>Test design:
<ul>
<li>The DAL has tests: <code>/tests/dal</code>. Mostly <code>/tests/dal/repository.py</code>.
<ul>
<li>These tests only use the DAL interface, meaning they only create and access data through the DAL, they
don&rsquo;t directly touch the DB.</li>
</ul>
</li>
<li>There are service-level tests: <code>/tests/api</code>.
<ul>
<li>They test the HTTP API using a &ldquo;TestClient&rdquo;, which is a utility provided by the
web framework we&rsquo;re using to allow us to simulate http requests to the service - so we &ldquo;send http requests&rdquo; to a
&ldquo;running server&rdquo; inside a test process, but there&rsquo;s no actual server running and the requsets are really just function
calls. This is a fairly standard utility in many backend frameworks.</li>
<li>These tests only operate at the HTTP API level - data is created by calling HTTP endpoints, for example, it&rsquo;s not
directly inserted into the DB.</li>
</ul>
</li>
<li>There are a bunch of other tests, but they&rsquo;re less interesting ATM and the feedback loop will actually be just fine
without most of them.</li>
</ul>
</li>
</ul>
<p>Here&rsquo;s a rough</p>
<p>Here&rsquo;s a short video (no sound) to show what</p>
<p>The idea is not to discuss any specific technique,</p>
<p>I&rsquo;m giving an example of a project that&rsquo;s very small,</p>
<p>In this post I&rsquo;ll focus on the reason I think frameworks are the direction and what I think they will include,
where following posts will dive deeper into the technical details.<br>
The objective here is not to &ldquo;convince&rdquo; (that would not fit in a blog post :) ), but to lay out the main points.</p>
<p>At a very high level, the rationale is this:</p>
<ol>
<li>It makes sense to &ldquo;aim our coding for the AI&rdquo;, because it&rsquo;ll be the new norm.</li>
<li>With the current way we&rsquo;re making software, it&rsquo;s just not realistic to make a paradigm shift happen, because AI
cannot have a feedback loop that&rsquo;s good enough to change code at an acceptable speed and self-heal enough issues.</li>
<li>Therefore, I believe the best forward is to have frameworks that dictate some important aspects of the code and
workflow, in order to give AI what it needs so it can do its job.</li>
</ol>
<p>Let&rsquo;s dig in.</p>
<h2 id="an-industrial-revolution">An industrial revolution<a hidden class="anchor" aria-hidden="true" href="#an-industrial-revolution">#</a></h2>
<p>Soon enough, the vast majority of code will be written by AI.<br>
So in a sense, this is a large automation movement.<br>
Things human craftsmen currently make will be handed over en masse to the machine.</p>
<p>Given that, it makes sense to think about the common case for software creation, which will soon be the automated
process.<br>
And as with other automations, the automation might involve a redesign.</p>
<p>The printing press is not a faster pen. An assembly line is not a faster human craftsman.
They create in a different way, and their outcome is a little different.<br>
And we should expect the same from automating software creation.</p>
<h2 id="the-size-of-an-iteration">The size of an iteration<a hidden class="anchor" aria-hidden="true" href="#the-size-of-an-iteration">#</a></h2>
<p>The shorter a feedback loop is, the better.<br>
Any mistake can send you off in the wrong direction, so you want to get feedback as early as possible.</p>
<p>A simple example that can help visualize why this is important would be to imagine writing code for an entire week
without ever running it, compiling it or getting warnings from the IDE.<br>
We all know that the result is going to be pretty nasty. No serious developer will choose to work like this.</p>
<p>This matters A LOT (personally, &ldquo;iteration size&rdquo; is the main &ldquo;metric&rdquo; I look at for improving productivity).</p>
<h2 id="ai-and-our-feedback-loops">AI and our feedback loops<a hidden class="anchor" aria-hidden="true" href="#ai-and-our-feedback-loops">#</a></h2>
<p>Although not always discussed this way, a lot of what we&rsquo;re doing now with AI tools is integrating them into our own
feedback loop.<br>
And when you look at it from this angle, you can see that a lot of the techniques we use to improve the performance of
human+ai are just optimizing some part of our feedback loop - adding steps in the process for detailed planning,
breaking down to smaller tasks, making sure the correct context gets in so it&rsquo;s easier for the AI to &ldquo;do&rdquo; etc.</p>
<h2 id="ais-internal-feedback-loop-today">AI&rsquo;s internal feedback loop today<a hidden class="anchor" aria-hidden="true" href="#ais-internal-feedback-loop-today">#</a></h2>
<p>An observation I&rsquo;m making here is that the AI has (or can have) its own feedback loop before it hands the latest change
over to us.</p>
<p>My own experience, and I&rsquo;m sure many others&rsquo; as well, is that the best outcome I get is if I set up an AI assistant for
a feedback loop. Each step looks like this:</p>
<ol>
<li>Understand what to do next</li>
<li>Create something small</li>
<li>Verify. For example, write a new test + Run all relevant tests. If something breaks - the next action would be to fix it.</li>
</ol>
<p>I let this run automatically in a loop until either all tests are green, it gives up or it starts going crazy.<br>
This is possible, for example with Cursor, for some time now. In fact, Cursor will try to self-heal what it can out of
the box - stuff like lint errors.</p>
<p>This works extremely well. The AI having its own internal self-healing feedback loop, backed by the right tests to make
me feel safe-enough that nothing breaks, is a different category of productivity.<br>
Some bugs still get through to me. I test, but it&rsquo;s kind of &ldquo;skipping to the end of the process&rdquo; - I would also do the
same tests if I wrote the code, and find similar bugs.</p>
<p>When I can set it up, it&rsquo;s just great. But&hellip; for most of my work, I can&rsquo;t.</p>
<h2 id="ais-internal-feedback-loop-tomorrow">AI&rsquo;s internal feedback loop tomorrow<a hidden class="anchor" aria-hidden="true" href="#ais-internal-feedback-loop-tomorrow">#</a></h2>
<p>When I distill the situation as far as I can, this is what remains.</p>
<ol>
<li>An LLM will statistically do the wrong thing sometimes.</li>
<li>Large, standard codebases and systems are FAR too complex for these &ldquo;imperfections&rdquo; to be rare.</li>
<li>So if we want to avoid most of these bugs reaching a human - we must allow the AI to self-correct using a feedback loop.</li>
</ol>
<p>A feedback loop is therefore sort-of a &ldquo;constant&rdquo; in this &ldquo;movement to AI&rdquo;. A requirement that must be satistified for
large, ongoing projects.</p>
<p>It follows that at least for complex projects (though I would say for almost all projects), our best bet is to adopt
coding practices that work well with such a feedback loop.</p>
<p>These kinds of coding practices are what I call AI-first frameworks.</p>
<h2 id="what-are-some-limiting-factors">What are some limiting factors?<a hidden class="anchor" aria-hidden="true" href="#what-are-some-limiting-factors">#</a></h2>
<p>So, why don&rsquo;t we have such feedback loops for our projects today?<br>
Why don&rsquo;t we just tell the AI &ldquo;here&rsquo;s a project with a million lines of code. Add a small feature&rdquo;, and then it would do
changes and verify them itself?</p>
<p>If you&rsquo;re following the trend on social media, you can see that improvements happen all the time, both in tools and how
to use them.</p>
<p>But if we look from the perspective of the plan-do-verify stages, we can see that almost everything falls between
&ldquo;plan&rdquo; and &ldquo;do&rdquo;.</p>
<p>There are definitely improvements in &ldquo;verify&rdquo; (and companies that emphasize quality*), but for the &ldquo;general project&rdquo;
there really is no big news. AI assistants will generate code and if you ask they&rsquo;ll generate some tests. But there&rsquo;s no
tool that you can throw on your project that would make changes and make sure nothing broke.</p>
<p>So</p>
<p>Twitter is full of tips and tricks on rule files and project documentation, workflows that break down work
into small steps etc.</p>
<p>The way I see it, we have two significant bottlenecks:<br>
First: understanding complex code well, incl. flow execution. I believe there are codebases where this works
pretty well, but certainly not consistently enough.<br>
And second: the real limiting factor of the AI software movement - &ldquo;verify&rdquo;.</p>
<p>The tests.<br>
Well, mostly the tests - there are other verifications.<br>
For example, automatically taking a one-time screenshot of a UI component &ldquo;before and after&rdquo; and comparing is very much
a verification, though it&rsquo;s not considered a &ldquo;test&rdquo; by common terminology. But
I&rsquo;ll usually include all verification in &ldquo;tests&rdquo; to be a little less verbose.</p>
<h2 id="the-tests">The tests<a hidden class="anchor" aria-hidden="true" href="#the-tests">#</a></h2>
<p>I&rsquo;m proficient with tests. I have given testing a lot of attention for two decades, I give talks about tests, I had
successes and failures and I have an &ldquo;arsenal&rdquo; of effective techniques.</p>
<p>Because of that, I know it&rsquo;s not realistic to add good tests to most code bases. At least, good tests that will be fast
enough.<br>
<strong>Code is usually not testable if it&rsquo;s not designed to be testable.</strong></p>
<p>Some common issues are</p>
<ol>
<li>Logical complexity, where some component does a lot of stuff and it&rsquo;s difficult to isolate what you want to test
in a way that&rsquo;s both reliable and simple enough. Especially for complex flows that include multiple steps and many
components with non-trivial state changes.</li>
<li>Technical complexity, where it&rsquo;s difficult to set up things. A simple example would be data samples for complex
processes, but the more hairy issues involve things like like relying on some external thing for which it&rsquo;s tricky to
simulate failure modes.</li>
<li>Side-effects, where the code interacts with the outside world in some way.</li>
</ol>
<p>Consider something like this: we have an arbitrary microservice architecture with 50 repos, where some services are written in typescript, some are written in python without type hints, and the team has even braved Rust on one of them. We mostly use Postgres with redis, but some workflows use s3. 5 3rd party APIs handle a few concerns like payments, opening support tickets or similar. We might not even have a trivial way to set up &ldquo;new clean test&rdquo; on a local machine. Now, the AI made a significant change to a shared python library that deals with the DB.<br>
When you call the typescript web service, which will then call 15 different services, how easy will it be for the us to trust that all read-only transactions stay read-only and that the latest change doesn&rsquo;t accidentally override a value in some obscure sequence of events where Stripe returns 502 once every 3 weeks?
We can&rsquo;t run all the options. It might not even be possible to technically test all failure modes for all 3rd party services that we use. And would it not run for 30 years if we tried?<br>
And since we&rsquo;re not testing it out, what will we do? Do you trust any model that tells you &ldquo;I&rsquo;ve seen all the context, there&rsquo;s no sequence of events that triggers a dormant bug from 2 years ago and breaks this&rdquo;?<br>
We need to trust it, or this doesn&rsquo;t work.</p>
<p>Now, there are solutions, but they require specific design patterns and tooling.</p>
<p>I&rsquo;ll finish this point here, because this is already a very long post.<br>
If what I wrote here has not convinced you - let&rsquo;s mark this as a debt to be explained, and one of the planned posts in
the series will take a deep-dive into this point.</p>
<p>For now, let&rsquo;s assume that adding good, fast tests to an arbitrary code base is very hard.<br>
Hard enough that to make the paradigm shift it&rsquo;s easier to switch coding styles than it is to create tools that can deal
with the old code style.</p>
<p>NOTE: maybe say something like
If you want to make sure that an endpoint is read only, you can do ad-hoc static analysis to try to make sure that there
is no code path where an sql query that changes data runs, you can write thousands of tests covering many possible
scenarios for what might have been in the DB to begin with etc.<br>
You can also use some permissions framework and only give that endpoint read-only access to the DB.
Then, in order to test that &ldquo;endpoint_x doesn&rsquo;t write to the DB&rdquo; we just need to make sure that we use the permission
framework and that the endpoint was indeed assigned read-only permissions.<br>
It&rsquo;s not a very common approach today, but it&rsquo;s easy to see how it eliminates categories of bugs very easily and very
reliably.</p>
<h2 id="security">Security<a hidden class="anchor" aria-hidden="true" href="#security">#</a></h2>
<p>I&rsquo;d be remiss if I didn&rsquo;t mention security.<br>
Even for something as simple as testing locally - the disk on our machine typically has a <code>.env</code> file with secrets.<br>
Automatically running a unit test that was created automatically, on code that was created automatically might leak that.</p>
<p>So if we want to be able to just let the AI do its thing with minimal supervision, it might be a good idea to have some guardrails.</p>
<h2 id="frameworks">Frameworks<a hidden class="anchor" aria-hidden="true" href="#frameworks">#</a></h2>
<p>It therefore seems that the most make-sense way forward, is to build AI-compatibility into every part of software
creation - design, coding, workflows, security etc.</p>
<p>If tests on a general codebase are hard, let&rsquo;s have a design that guarantees our codebases can be tested.<br>
If we need to avoid leaking information, we probably need to address that.</p>
<p>It&rsquo;s difficult enough to solve these issues if we &ldquo;own&rdquo; the tech stack and design, and can freely run
any part of the code that we want.<br>
If we don&rsquo;t, it just becomes exponentially more difficult.</p>
<p>We need to understand what AI needs, and give it that, instead of trying to have it &ldquo;just work&rdquo; on whatever arbitrary
code base we happen to have.</p>
<p>This is what I think of as an AI-native framework.<br>
It would take a certain use case (hopefully relatively broad) and design some of the of the aspects of the project to
make them AI-compatible.</p>
<p>I don&rsquo;t know what would be the granularity of these frameworks</p>
<ul>
<li>Maybe it&rsquo;ll be a single idea like testing at a certain layer in an http web server and you would compose a bunch of
these together</li>
<li>Maybe it&rsquo;ll be &ldquo;FastAPI + Postgres + React has at least these layers and tests that look like these&rdquo;</li>
<li>Maybe it&rsquo;ll be much more generic than that.</li>
<li>Maybe the only frameworks we see will be part of paid platforms that also contain the AI engines that use the frameworks. You could satisfy a lot of these requirements by having something like Wix and have custom-generated-code hook into different parts of the system. I don&rsquo;t think this is where it&rsquo;s going, but maybe.</li>
</ul>
<p>What I am sure of is that frameworks will set up software projects so that AI engines can have effective feedback
loops.<br>
They will include at least some strict design and testing guidelines, and will probably include ways to document and declare a
spec for parts of the software.</p>
<p>I do believe the successful ones will probably be technology-specific, at least to some degree.<br>
There are things we can do with some technologies that we can&rsquo;t do with others, and they matter.<br>
For example, we don&rsquo;t have static types in javascript (put aside jsdoc for a sec), but we do have them in typescript. This
changes some tradeoffs of what&rsquo;s easy for the AI to self-heal and what is not.</p>
<p>I believe some of the conventions we see in these frameworks will be very similar to what some teams already do.<br>
But some will not. It is easier for a machine to do some things that are difficult for us and vice versa.<br>
For example, the way it makes sense for an AI to use types is typescript is not the same in my opinion as the common
conventios. I&rsquo;ll dig into this quite a bit later on.</p>
<p>Some tooling will almost certainly be very different from today.<br>
I don&rsquo;t think we&rsquo;ll have Python code generation at scale without at least something like a sandbox be part of common
frameworks. Maybe even the ability to easily configure file access, limitations on network calling etc.</p>
<h2 id="wrapping-up">Wrapping up<a hidden class="anchor" aria-hidden="true" href="#wrapping-up">#</a></h2>
<p>I hope I managed to make the case that it&rsquo;ll very difficult to have industrial-strength code generation at scale for arbitrary code bases.<br>
And that we must therefore explore frameworks that would allow us to control enough of the structure of the code
that would allow the AI to have an effective internal feedback loop.</p>
<p>In the next post, we&rsquo;ll start exploring an example of what a simple framework might look like.</p>
<ul>
<li>About companies that emphasize quality - I&rsquo;m biased, as I worked there in the past, but if you&rsquo;re interested you might
want to have a look at Qodo.</li>
</ul>
<hr>

<div style="text-align: center; display: block; width: 100%;">
<a href="/posts/ai_frameworks/01_ai_frameworks_intro">&lt;&lt; previous post: AI-First Development Frameworks (intro)</a>
|
next post: (coming soon) &gt;&gt;
</div>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://shaigeva.com/">Shai Geva</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
