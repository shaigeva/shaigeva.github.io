<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>AI-first design patterns - CRUD backend fundamentals | Shai Geva</title>
<meta name="keywords" content="AI, AI-first, AI-driven, AI-first design-patterns, design-patterns, testing, backend, API, DAL">
<meta name="description" content="
this post is part of a series about creating
production-grade maintainable AI-first projects, using AI-first design patterns and frameworks.


One of the best ways to help our AI agent work on our code is to set it up with a solid feedback
loop so it can &ldquo;plan -&gt; do -&gt; verify&rdquo; on its own.
We define the design, and the agent follows our conventions to create code and tests that enable the feedback loop.
(that&rsquo;s the premise of this post; earlier posts in the series go deeper on the rationale).">
<meta name="author" content="Shai Geva">
<link rel="canonical" href="https://shaigeva.com/posts/ai_frameworks/03_simple_ai_first_crud_backend_design_pattern/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ad7e9ed672bfa63aa62f2a9523f793e89f751a105296a152608bd1ccabf63b08.css" integrity="sha256-rX6e1nK/pjqmLyqVI/eT6J91GhBSlqFSYIvRzKv2Owg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shaigeva.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shaigeva.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shaigeva.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shaigeva.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://shaigeva.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://shaigeva.com/posts/ai_frameworks/03_simple_ai_first_crud_backend_design_pattern/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-YWC9X8YE9C"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-YWC9X8YE9C');
        }
      </script><meta property="og:url" content="https://shaigeva.com/posts/ai_frameworks/03_simple_ai_first_crud_backend_design_pattern/">
  <meta property="og:site_name" content="Shai Geva">
  <meta property="og:title" content="AI-first design patterns - CRUD backend fundamentals">
  <meta property="og:description" content=" this post is part of a series about creating production-grade maintainable AI-first projects, using AI-first design patterns and frameworks. One of the best ways to help our AI agent work on our code is to set it up with a solid feedback loop so it can “plan -&gt; do -&gt; verify” on its own.
We define the design, and the agent follows our conventions to create code and tests that enable the feedback loop.
(that’s the premise of this post; earlier posts in the series go deeper on the rationale).">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-04T13:01:56+03:00">
    <meta property="article:modified_time" content="2025-11-04T13:01:56+03:00">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="AI-First">
    <meta property="article:tag" content="AI-Driven">
    <meta property="article:tag" content="AI-First Design-Patterns">
    <meta property="article:tag" content="Design-Patterns">
    <meta property="article:tag" content="Testing">
    <meta property="og:image" content="https://shaigeva.com/ai_frameworks/robot_thinking_crud.jpg">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://shaigeva.com/ai_frameworks/robot_thinking_crud.jpg">
<meta name="twitter:title" content="AI-first design patterns - CRUD backend fundamentals">
<meta name="twitter:description" content="
this post is part of a series about creating
production-grade maintainable AI-first projects, using AI-first design patterns and frameworks.


One of the best ways to help our AI agent work on our code is to set it up with a solid feedback
loop so it can &ldquo;plan -&gt; do -&gt; verify&rdquo; on its own.
We define the design, and the agent follows our conventions to create code and tests that enable the feedback loop.
(that&rsquo;s the premise of this post; earlier posts in the series go deeper on the rationale).">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://shaigeva.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "AI-first design patterns - CRUD backend fundamentals",
      "item": "https://shaigeva.com/posts/ai_frameworks/03_simple_ai_first_crud_backend_design_pattern/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "AI-first design patterns - CRUD backend fundamentals",
  "name": "AI-first design patterns - CRUD backend fundamentals",
  "description": " this post is part of a series about creating production-grade maintainable AI-first projects, using AI-first design patterns and frameworks. One of the best ways to help our AI agent work on our code is to set it up with a solid feedback loop so it can \u0026ldquo;plan -\u0026gt; do -\u0026gt; verify\u0026rdquo; on its own.\nWe define the design, and the agent follows our conventions to create code and tests that enable the feedback loop.\n(that\u0026rsquo;s the premise of this post; earlier posts in the series go deeper on the rationale).\n",
  "keywords": [
    "AI", "AI-first", "AI-driven", "AI-first design-patterns", "design-patterns", "testing", "backend", "API", "DAL"
  ],
  "articleBody": " this post is part of a series about creating production-grade maintainable AI-first projects, using AI-first design patterns and frameworks. One of the best ways to help our AI agent work on our code is to set it up with a solid feedback loop so it can “plan -\u003e do -\u003e verify” on its own.\nWe define the design, and the agent follows our conventions to create code and tests that enable the feedback loop.\n(that’s the premise of this post; earlier posts in the series go deeper on the rationale).\nTurns out that if our code is a backend API service, there are simple design and testing patterns that have a great ROI.\nWe don’t need to invent anything new; there are well-known, established options. We just need to pick ones that are a good fit for creating an AI-internal feedback loop.\nThis post looks at two classic approaches that work well.\nYou probably already know them, but to make them effective for an agent we need to make some specific implementation choices.\nWe’ll walk through those choices, explain why they work when others don’t, and dive into concrete low-level details to help with implementation.\nAfter covering the patterns, we’ll look at an example CRUD backend that applies them and close with takeaways from building the service.\nAt high level For those who already speak testing lingo and just want the headline, here’s the short, unexplained list of the patterns and the key choices that make them effective:\nDesign pattern #1: Rely heavily on HTTP API tests. Use them to cover as much functionality as possible. Key choices:\nAPI-level really means API-level: everything goes through HTTP, including test-data setup. Make the DB fast enough for tests (in-memory, fakes, local DB etc.), even if it takes effort. Use an in-process TestClient if possible. Design pattern #2: Also have a DAL and test it thoroughly (optional but strongly recommended). Key choices:\nDAL-level means DAL-level: interact only through the DAL interface, including test-data setup. Use the same DB considerations as with the API tests. This post focuses on a local CRUD backend service.\nThe covered patterns can be thought of as “fundamentals for a backend service”; they form the base that we can build on.\nOther concerns, like external dependencies, are obviously important but we can only cover so much in one post so they’ll have to wait their turn.\nWhat’s the basic goal? To make the feedback loop effective, the agent needs to check itself.\nFor that to work, the tests must be “good” in specific ways:\nClose enough to the truth: pass/fail should usually mean no-bug/bug. Extensive enough: broad coverage of the service’s behavior. Fast enough: runnable after small code changes. Notice that each point contains “enough”.\nIt has to be good, but it doesn’t need to be perfect; if the agent catches most bugs quickly, that’s already a 10x boost.\nHow is this different from normal testing? On one hand, it’s not very different - these same considerations are important in normal testing as well.\nBut agents make them more important, because getting the tests right can be the difference between the agent having a decent feedback loop and not having it.\nThe agent relies on the tests for its quick feedback.\nIf they’re unreliable, not extensive, or too slow to run after small changes, the agent won’t get the feedback it needs. It’ll go off in the wrong direction without any way to notice, and a lot of the verification work will fall back on us humans, which defeats the purpose.\nAgents also shift the tradeoffs.\nWriting code and tests is much cheaper now; we can have a thousand where we earlier had ten.\nThe hard part is making sure they exercise something real, so investing in a setup that biases tests toward quality now has a much higher ROI.\nDesign pattern #1: Heavily rely on API tests This is the most important (and most obvious) idea.\nIt works with any code design, as long as you have a way to handle the DB.\nThe “truth” about how an API service behaves is its HTTP interface.\nIf it receives HTTP requests and returns the right responses, it works.\nSo obviously, this is the most reliable way to test it: tests that interact with the service as if they are an external client.\nThese are sometimes referred to as integration tests, but that's a fairly vague term so I'll stick with the more explicit \"API tests\". This means that in many cases, the default strategy for testing a service is to lean on API tests:\nBuild a solid setup for the API tests. Have the agent write thorough tests for all possible functionality through these API tests. It won’t cover everything, but if this is the starting point, you’re already in a strong place.\nThe tricky part is building a solid setup for the tests.\nThere are many ways to write API tests, and we need to pick the ones that avoid the pitfalls that would undermine the feedback loop.\nPerformance pitfalls The best option is to have the API tests run as in-process unit-tests, with the DB being in-memory.\nHow can HTTP API tests be in-process? Many web frameworks have a “TestClient” that allows simulating HTTP requests.\nYou get the exact same interface as a real HTTP client, and the framework sets it up to be functionally identical (or close-enough).\nExample (Python - FastAPI + pytest):\ndef test_create_user_as_super_admin(self, client: TestClient) -\u003e None: response = client.post( \"/api/users\", params={\"organization_id\": org_id, \"role\": \"admin\"}, json={\"username\": \"newuser\", \"email\": \"new@example.com\", \"full_name\": \"New User\"}, headers=auth_headers(super_admin_token), ) assert response.status_code == 201 ... The Pythonists among you may notice that client.post looks exactly like the popular requests.post function.\nSo we stay within the same process and use simple function calls instead of having a separate process with an actual HTTP server.\nVery fast and still reliable-enough.\nWhat about the DB? The thing we have to have for the feedback loop is “the tests must be fast-enough and reliable-enough most of the time”.\nSo yes - it’s a non-trivial challenge, but there’s a lot of wiggle room.\nHere are some options on the DB front:\nIn-memory mode: use it if your DB supports it. Use a fake DB: a fake is a simulator that behaves like the real thing for tests. For example, an in-memory DB (e.g. SQLite) might stand in for Postgres with a small adapter. Set up your DB so it’s local and fast enough. Add utilities that keep the DB fast: E.g. for local Postgres, bake everything, including test data, into a docker image and spin up a fresh container per test suite for a clean, instant DB. Explore caching, pre-calculation, parallelization, etc. Set up the agent so most of the time, it only runs a small subset of the tests that are related to the changed code. Fake the DAL: if the other solutions don’t cut it, then your only option might be to use a DAL (Data Access Layer - which is a good idea anyway, see below) and create an alternative DAL implementation that doesn’t use your DB. For example, it can use simple data structures that behave like the DB in tests. It takes more work (mostly for the agent) and is less reliable, but if that’s the only option, then that’s life. Keep both a “fast mode” (e.g. in-memory SQLite) and a “reliable mode” (real Postgres), and run the reliable mode only occasionally. The core idea is to be pragmatic: be good enough most of the time.\nThat’s enough for a decent feedback loop.\nCorrectness pitfalls These points are true in any testing effort, but agents amplify them.\nCorrectness is tricky because plenty of popular “best practices” just won’t work here.\nBecause the only real dependency in a CRUD is the DB, the main correctness issue is how we create and access the thing that lives in the DB: the test data.\nThe right thing to do is to create and access test data through the interface you’re exercising.\nFor API tests, behave exactly like an external client:\nGenerate all test data through the layer that we’re testing - in this case the HTTP interface itself. Verify the results - again, though the same interface. Don’t use the ORM, the DAL, mocks, stubs, or any other internal shortcut.\nIf you do, the data can drift from reality and the tests become unreliable.\nSubtle bugs creep in; for example, asserting on a raw DB value that the system uses in a different way than expected.\nBecause the agent relies almost completely on these tests for feedback, it has no way to notice it’s creating bugs.\nMake the effort; it’s the best investment you can make.\nIf you’d like a more concrete look, I mentioned this at a PyCon US talk (in a non-AI context, but everything still applies), and following that talk wrote a blog post about it.\nDesign pattern #2: Have a DAL with its own strong tests What’s a DAL? A common pattern when working with DBs is to add a DAL (Data Access Layer, sometimes called a repository) that abstracts and encapsulates DB access.\nThe DAL exposes a small set of actions against the DB, and the rest of the service only uses those actions.\nThey take and return domain-model objects (immutable data structures representing entities like User, Project, Ticket).\nCode outside the DAL doesn’t have access to the DB or the ORM. It only knows the domain-model objects and the DAL handles the DB itself.\nFor example, creating a User might look like this:\ndef update_user( self, user_id: str, update_command: UserUpdateCommand, # INPUT - simple data structure, not connected to DB ) -\u003e Optional[User]: # DB ACCESS is here: orm_user = self.session.query(UserORM).filter(UserORM.id == user_id).first() # ... # Where the UserUpdateCommand is just a data structure: class UserUpdateCommand(BaseModel): email: Optional[EmailStr] = Field(None, description=\"User email address\") full_name: Optional[str] = Field(None, min_length=1, max_length=255, description=\"User full name\") role: Optional[UserRole] = Field(None, description=\"User role\") is_active: Optional[bool] = Field(None, description=\"Whether user is active\") Why have a DAL + tests? A DAL has downsides (like code bloat). We still want it because it gives the agent a consistent way to make the code more robust:\nIt’s possible for the agent to work on it effectively: we can set up a feedback loop around a DAL because it can be tested on its own. The API tests by themselves might not be enough, and strong DAL tests add reliability around the DB access, which is typically one of the most error-prone areas. Design-wise, this encapsulation usually improves maintainability. Two more subtle benefits:\nClearer context makes it easier for the agent to focus: Outside the DAL, the only DB context the agent needs is the narrow DAL interface. Without a DAL, DB access spreads across many files and patterns, making it harder to surface the relevant details. Inside the DAL, the agent mostly needs the rest of the interface plus the DB logic contained within the DAL. Protect the feedback loop from future disasters: As mentioned above, if you don’t have a fast-enough DB setup, you might be forced to have a DAL and create a fake (simulator) for it. BUT, even if you do have a fast DB setup TODAY - in 6 months you might need to change the DB for some of your data for scale reasons. And the new DB might not have a fast-enough setup. So if you don’t start with a DAL today, you will be forced to create a new DAL and a fake for it or say goodbye to your agent’s feedback loop. But by then, you might have thousands of lines of code that access the DB directly, and you’ll need to refactor all that sensitive code into the new DAL - which might be very expensive and introduce bugs. Don’t risk it. What choices do we need to make for the DAL tests? So we want to have a DAL, and we want to have our agent write thorough tests for it.\nThe setup is similar to the API tests, so the same pitfalls apply:\nRun everything through the DAL’s external interface (the repository API that uses domain models), not the ORM or other internal APIs. Keep the tests fast; the DB considerations are similar to the API test setup. Summing up our patterns Because API-level tests are the “truth”, a fast and reliable DB setup makes them enough to give the agent a solid feedback loop.\nIn my recent experience, this works so well that I wouldn’t start a new service without them.\nFor a small CRUD, API tests might be sufficient, but I still recommend adding a DAL with thorough tests.\nYes, it’s more code, another abstraction, and some overlap, but the agent handles it fine (often better, thanks to clearer context) and the system ends up sturdier and easier to maintain.\nExample project To make this concrete, I created a small-ish CRUD backend project that applies the design patterns above.\nIf you’d like to see them in practice, browse the code (links to specific files below).\nIt’s a Python backend for a basic project management app with:\nOrganizations (tenants) with isolated data Role-based access control (Admin, Project Manager, Write Access, Read Access) Projects containing tickets with configurable workflows Epics that span multiple projects Comments on tickets Activity logs and audit trails with permission-based access There’s a rough UI covering part of the functionality. It’s not part of the workflow I’m describing; I just created it to get a feel that the API is usable:\nProject structure This project was created pretty much entirely by an AI agent (Claude Code, with guidance of course) - I set up scaffolding and agent rules for coding conventions and the feedback loop, and the rest was generated by the agent in several sessions.\nThe stack uses common Python tools such as FastAPI and SQLAlchemy.\nIt’s a local-only SQLite setup with a few thousand lines of code, enough to show the design on something richer than “Hello, World”.\nThe structure mirrors what we’ve discussed. Examples:\nOne of the API routers: https://github.com/shaigeva/project_management_crud_example/blob/main/project_management_crud_example/routers/ticket_api.py DAL: https://github.com/shaigeva/project_management_crud_example/blob/main/project_management_crud_example/dal/sqlite/repository.py Tests: One of the API tests: https://github.com/shaigeva/project_management_crud_example/blob/main/tests/api/test_ticket_api.py One of the DAL tests: https://github.com/shaigeva/project_management_crud_example/blob/main/tests/dal/test_ticket_repository.py In addition to the API and DAL tests, there are some unit tests for specific modules, in cases that a module can be tested in isolation without mocks etc. Tests: some extra details Spinning up a fresh DB is quick here, so each test gets a new physical SQLite file in a temp directory. We seed it with test data and discard it when the test finishes.\nThe full suite takes ~20 seconds.\nThat’s fine for an example, but in a larger project I’d default to in-memory runs, cache more of the setup, and have the agent run only the tests related to the change so typical runs stay in the several-second range.\nHonorary mention: Spec-driven development The focus of this post is on code \u0026 tests design, so I won’t get into the details here, but it’s worth a mention that workflow-wise, the project is spec-driven in spirit.\nIt follows a ~consistent pattern to move from high-level spec to low-level implementation, and this has been a major part in getting the agent to do consistent, focused work.\n┌──────────────────────────────────┐ │ High-Level Spec │ (main features) └────────────────┬─────────────────┘ │ ▼ ┌──────────────────────────────────┐ │ Detailed Specs │ (persistent - requirements \u0026 acceptance └────────────────┬─────────────────┘ criteria for specific features) │ ▼ ┌──────────────────────────────────┐ │ Current Feature Implementation │ (ephemeral - archived when done) │ Plan │ └────────────────┬─────────────────┘ │ ▼ ┌──────────────────────────────────┐ │ Code and Tests │ └──────────────────────────────────┘ Summing up My goal with this post was to highlight foundational design and testing patterns for API services and show why they are better than others for AI-first development.\nI shared the details to keep things concrete enough to try.\nIn my experiments, these patterns perform very well, and I hope they’re useful to you, too.\nI’d love to hear your thoughts on social (twitter / x, linkedin)!\n\u003c\u003c previous post: What an AI Feedback Loop Looks Like | next post: (coming soon) \u003e\u003e ",
  "wordCount" : "2680",
  "inLanguage": "en",
  "image":"https://shaigeva.com/ai_frameworks/robot_thinking_crud.jpg","datePublished": "2025-11-04T13:01:56+03:00",
  "dateModified": "2025-11-04T13:01:56+03:00",
  "author":{
    "@type": "Person",
    "name": "Shai Geva"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shaigeva.com/posts/ai_frameworks/03_simple_ai_first_crud_backend_design_pattern/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shai Geva",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shaigeva.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shaigeva.com/" accesskey="h" title="Shai Geva (Alt + H)">Shai Geva</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shaigeva.com/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://shaigeva.com/talks" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="https://shaigeva.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      AI-first design patterns - CRUD backend fundamentals
    </h1>
    <div class="post-meta"><span title='2025-11-04 13:01:56 +0300 +0300'>November 4, 2025</span>&nbsp;·&nbsp;<span>Shai Geva</span>

</div>
  </header> 
<figure class="entry-cover">
        <img loading="eager" src="https://shaigeva.com/ai_frameworks/robot_thinking_crud.jpg" alt="">
        
</figure>
  <div class="post-content"><span class="aside">
this post is part of a <a href="/posts/ai_frameworks/01_ai_frameworks_intro" target="_blank">series</a> about creating
production-grade maintainable AI-first projects, using AI-first design patterns and frameworks.
</span>

<p>One of the best ways to help our AI agent work on our code is to set it up with a solid feedback
loop so it can &ldquo;plan -&gt; do -&gt; verify&rdquo; on its own.<br>
We define the design, and the agent follows our conventions to create code and tests that enable the feedback loop.<br>
(that&rsquo;s the premise of this post; earlier posts in the series go deeper on the rationale).</p>
<p>Turns out that if our code is a backend API service, there are simple design and testing patterns that have
a great ROI.<br>
We don&rsquo;t need to invent anything new; there are well-known, established options. We just need to pick
ones that are a good fit for creating an AI-internal feedback loop.</p>
<p>This post looks at two classic approaches that work well.<br>
You probably already know them, but to make them effective for an agent we need to make some specific implementation
choices.<br>
We&rsquo;ll walk through those choices, explain why they work when others don&rsquo;t, and dive into concrete low-level details to
help with implementation.</p>
<p>After covering the patterns, we&rsquo;ll look at an example CRUD backend that applies them and close with takeaways from
building the service.</p>
<h1 id="at-high-level">At high level<a hidden class="anchor" aria-hidden="true" href="#at-high-level">#</a></h1>
<p>For those who already speak testing lingo and just want the headline, here&rsquo;s the short, unexplained list of the patterns and
the key choices that make them effective:</p>
<p>Design pattern #1: Rely heavily on HTTP API tests. Use them to cover as much functionality as possible. Key choices:</p>
<ul>
<li>API-level really means API-level: everything goes through HTTP, including test-data setup.</li>
<li>Make the DB fast enough for tests (in-memory, fakes, local DB etc.), even if it takes effort.</li>
<li>Use an in-process TestClient if possible.</li>
</ul>
<p>Design pattern #2: Also have a DAL and test it thoroughly (optional but strongly recommended). Key choices:</p>
<ul>
<li>DAL-level means DAL-level: interact only through the DAL interface, including test-data setup.</li>
<li>Use the same DB considerations as with the API tests.</li>
</ul>
<p>This post focuses on a local CRUD backend service.<br>
The covered patterns can be thought of as &ldquo;fundamentals for a backend service&rdquo;; they form the base that we can build on.<br>
Other concerns, like external dependencies, are obviously important but we can only cover so much in one post so they&rsquo;ll
have to wait their turn.</p>
<p><img alt="High-Level Architecture - without details" loading="lazy" src="/ai_frameworks/simple_crud_high_level_architechture_without_details.png"></p>
<h1 id="whats-the-basic-goal">What&rsquo;s the basic goal?<a hidden class="anchor" aria-hidden="true" href="#whats-the-basic-goal">#</a></h1>
<p>To make the feedback loop effective, the agent needs to check itself.<br>
For that to work, the tests must be &ldquo;good&rdquo; in specific ways:</p>
<ul>
<li>Close enough to the truth: pass/fail should usually mean no-bug/bug.</li>
<li>Extensive enough: broad coverage of the service&rsquo;s behavior.</li>
<li>Fast enough: runnable after small code changes.</li>
</ul>
<p>Notice that each point contains &ldquo;enough&rdquo;.<br>
It has to be good, but it doesn&rsquo;t need to be perfect; if the agent catches most bugs quickly, that&rsquo;s already a 10x boost.</p>
<h2 id="how-is-this-different-from-normal-testing">How is this different from normal testing?<a hidden class="anchor" aria-hidden="true" href="#how-is-this-different-from-normal-testing">#</a></h2>
<p>On one hand, it&rsquo;s not very different - these same considerations are important in normal testing as well.</p>
<p>But agents make them more important, because getting the tests right can be the difference between the agent having a
decent feedback loop and not having it.</p>
<p>The agent relies on the tests for its quick feedback.<br>
If they&rsquo;re unreliable, not extensive, or too slow to run after small changes, the agent won&rsquo;t get the
feedback it needs. It&rsquo;ll go off in the wrong direction without any way to notice, and a lot of the verification work
will fall back on us humans, which defeats the purpose.</p>
<p>Agents also shift the tradeoffs.<br>
Writing code and tests is much cheaper now; we can have a thousand where we earlier had ten.<br>
The hard part is making sure they exercise something real, so investing in a setup that biases tests toward quality now
has a much higher ROI.</p>
<h1 id="design-pattern-1-heavily-rely-on-api-tests">Design pattern #1: Heavily rely on API tests<a hidden class="anchor" aria-hidden="true" href="#design-pattern-1-heavily-rely-on-api-tests">#</a></h1>
<p>This is the most important (and most obvious) idea.<br>
It works with any code design, as long as you have a way to handle the DB.</p>
<p>The &ldquo;truth&rdquo; about how an API service behaves is its HTTP interface.<br>
If it receives HTTP requests and returns the right responses, it works.<br>
So obviously, this is the most reliable way to test it: tests that interact with the service as if they are an external
client.</p>
<span class="aside">
These are sometimes referred to as integration tests, but that's a fairly vague term so I'll stick with the more
explicit "API tests".
</span>

<p>This means that in many cases, the default strategy for testing a service is to lean on API tests:</p>
<ul>
<li>Build a solid setup for the API tests.</li>
<li>Have the agent write thorough tests for all possible functionality through these API tests.</li>
</ul>
<p><img alt="High-Level Architecture" loading="lazy" src="/ai_frameworks/simple_crud_high_level_architechture_only_api_tests_details.png"></p>
<p>It won&rsquo;t cover everything, but if this is the starting point, you&rsquo;re already in a strong place.</p>
<p>The tricky part is building a solid setup for the tests.<br>
There are many ways to write API tests, and we need to pick the ones that avoid the pitfalls
that would undermine the feedback loop.</p>
<h2 id="performance-pitfalls">Performance pitfalls<a hidden class="anchor" aria-hidden="true" href="#performance-pitfalls">#</a></h2>
<p>The best option is to have the API tests run as in-process unit-tests, with the DB being in-memory.</p>
<h3 id="how-can-http-api-tests-be-in-process">How can HTTP API tests be in-process?<a hidden class="anchor" aria-hidden="true" href="#how-can-http-api-tests-be-in-process">#</a></h3>
<p>Many web frameworks have a &ldquo;TestClient&rdquo; that allows simulating HTTP requests.<br>
You get the exact same interface as a real HTTP client, and the framework sets it up to be functionally identical
(or close-enough).<br>
Example (Python - FastAPI + pytest):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_create_user_as_super_admin</span>(self, client: TestClient) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>post(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;/api/users&#34;</span>,
</span></span><span style="display:flex;"><span>        params<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;organization_id&#34;</span>: org_id, <span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;admin&#34;</span>},
</span></span><span style="display:flex;"><span>        json<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;username&#34;</span>: <span style="color:#e6db74">&#34;newuser&#34;</span>, <span style="color:#e6db74">&#34;email&#34;</span>: <span style="color:#e6db74">&#34;new@example.com&#34;</span>, <span style="color:#e6db74">&#34;full_name&#34;</span>: <span style="color:#e6db74">&#34;New User&#34;</span>},
</span></span><span style="display:flex;"><span>        headers<span style="color:#f92672">=</span>auth_headers(super_admin_token),
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> response<span style="color:#f92672">.</span>status_code <span style="color:#f92672">==</span> <span style="color:#ae81ff">201</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span></code></pre></div><p>The Pythonists among you may notice that <code>client.post</code> looks exactly like the popular <code>requests.post</code> function.</p>
<p>So we stay within the same process and use simple function calls instead of having a separate process with an actual
HTTP server.<br>
Very fast and still reliable-enough.</p>
<h3 id="what-about-the-db">What about the DB?<a hidden class="anchor" aria-hidden="true" href="#what-about-the-db">#</a></h3>
<p>The thing we have to have for the feedback loop is &ldquo;the tests must be fast-enough and reliable-enough most of the
time&rdquo;.<br>
So yes - it&rsquo;s a non-trivial challenge, but there&rsquo;s a lot of wiggle room.<br>
Here are some options on the DB front:</p>
<ul>
<li>In-memory mode: use it if your DB supports it.</li>
<li>Use a fake DB: a fake is a simulator that behaves like the real thing for tests. For example, an in-memory
DB (e.g. SQLite) might stand in for Postgres with a small adapter.</li>
<li>Set up your DB so it&rsquo;s local and fast enough.</li>
<li>Add utilities that keep the DB fast:
<ul>
<li>E.g. for local Postgres, bake everything, including test data, into a docker image and spin up a fresh container per test
suite for a clean, instant DB.</li>
<li>Explore caching, pre-calculation, parallelization, etc.</li>
</ul>
</li>
<li>Set up the agent so most of the time, it only runs a small subset of the tests that are related to the changed code.</li>
<li>Fake the DAL: if the other solutions don&rsquo;t cut it, then your only option might be to use a DAL (Data Access
Layer - which is a good idea anyway, see below) and create an alternative DAL implementation that doesn&rsquo;t use your DB.
For example, it can use simple data structures that behave like the DB in tests. It takes more work (mostly for the
agent) and is less reliable, but if that&rsquo;s the only option, then that&rsquo;s life.</li>
<li>Keep both a &ldquo;fast mode&rdquo; (e.g. in-memory SQLite) and a &ldquo;reliable mode&rdquo; (real Postgres), and run the reliable mode
only occasionally.</li>
</ul>
<p>The core idea is to be pragmatic: be good enough most of the time.<br>
That&rsquo;s enough for a decent feedback loop.</p>
<h2 id="correctness-pitfalls">Correctness pitfalls<a hidden class="anchor" aria-hidden="true" href="#correctness-pitfalls">#</a></h2>
<p>These points are true in any testing effort, but agents amplify them.</p>
<p>Correctness is tricky because plenty of popular &ldquo;best practices&rdquo; just won&rsquo;t work here.</p>
<p>Because the only real dependency in a CRUD is the DB, the main correctness issue is how we create and access the thing
that lives in the DB: the test data.</p>
<p>The right thing to do is to create and access test data through the interface you&rsquo;re exercising.<br>
For API tests, behave exactly like an external client:</p>
<ul>
<li>Generate all test data through the layer that we&rsquo;re testing - in this case the HTTP interface itself.</li>
<li>Verify the results - again, though the same interface.</li>
</ul>
<p>Don&rsquo;t use the ORM, the DAL, mocks, stubs, or any other internal shortcut.</p>
<p>If you do, the data can drift from reality and the tests become unreliable.<br>
Subtle bugs creep in; for example, asserting on a raw DB value that the system uses in a different way than expected.<br>
Because the agent relies almost completely on these tests for feedback, it has no way to notice it&rsquo;s creating bugs.</p>
<p>Make the effort; it&rsquo;s the best investment you can make.</p>
<p>If you&rsquo;d like a more concrete look, I

<a href="https://youtu.be/Ub31Ae6S1BY?t=447" target="_blank">mentioned this at a PyCon US talk</a>
(in a non-AI context, but everything still applies), and following that talk
<a href="/posts/10_footguns/07_improper_test_scope" target="_blank">wrote a blog post</a>

about it.</p>
<h1 id="design-pattern-2-have-a-dal-with-its-own-strong-tests">Design pattern #2: Have a DAL with its own strong tests<a hidden class="anchor" aria-hidden="true" href="#design-pattern-2-have-a-dal-with-its-own-strong-tests">#</a></h1>
<h2 id="whats-a-dal">What&rsquo;s a DAL?<a hidden class="anchor" aria-hidden="true" href="#whats-a-dal">#</a></h2>
<p>A common pattern when working with DBs is to add a DAL (Data Access Layer, sometimes called a repository) that
abstracts and encapsulates DB access.</p>
<p>The DAL exposes a small set of actions against the DB, and the rest of the service only uses those actions.<br>
They take and return domain-model objects (immutable data structures representing entities like User, Project, Ticket).<br>
Code outside the DAL doesn&rsquo;t have access to the DB or the ORM. It only knows the domain-model objects and the DAL handles
the DB itself.</p>
<p><img alt="High-Level Architecture" loading="lazy" src="/ai_frameworks/simple_crud_high_level_architechture_with_details.png"></p>
<p>For example, creating a User might look like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update_user</span>(
</span></span><span style="display:flex;"><span>  self,
</span></span><span style="display:flex;"><span>  user_id: str,
</span></span><span style="display:flex;"><span>  update_command: UserUpdateCommand,  <span style="color:#75715e"># INPUT - simple data structure, not connected to DB</span>
</span></span><span style="display:flex;"><span>  ) <span style="color:#f92672">-&gt;</span> Optional[User]:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># DB ACCESS is here:</span>
</span></span><span style="display:flex;"><span>    orm_user <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>session<span style="color:#f92672">.</span>query(UserORM)<span style="color:#f92672">.</span>filter(UserORM<span style="color:#f92672">.</span>id <span style="color:#f92672">==</span> user_id)<span style="color:#f92672">.</span>first()
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Where the UserUpdateCommand is just a data structure:</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">UserUpdateCommand</span>(BaseModel):
</span></span><span style="display:flex;"><span>    email: Optional[EmailStr] <span style="color:#f92672">=</span> Field(<span style="color:#66d9ef">None</span>, description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;User email address&#34;</span>)
</span></span><span style="display:flex;"><span>    full_name: Optional[str] <span style="color:#f92672">=</span> Field(<span style="color:#66d9ef">None</span>, min_length<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">255</span>, description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;User full name&#34;</span>)
</span></span><span style="display:flex;"><span>    role: Optional[UserRole] <span style="color:#f92672">=</span> Field(<span style="color:#66d9ef">None</span>, description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;User role&#34;</span>)
</span></span><span style="display:flex;"><span>    is_active: Optional[bool] <span style="color:#f92672">=</span> Field(<span style="color:#66d9ef">None</span>, description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Whether user is active&#34;</span>)
</span></span></code></pre></div><h2 id="why-have-a-dal--tests">Why have a DAL + tests?<a hidden class="anchor" aria-hidden="true" href="#why-have-a-dal--tests">#</a></h2>
<p>A DAL has downsides (like code bloat).
We still want it because it gives the agent a consistent way to make the code more robust:</p>
<ol>
<li>It&rsquo;s possible for the agent to work on it effectively: we can set up a feedback loop around a DAL because it can be
tested on its own.</li>
<li>The API tests by themselves might not be enough, and strong DAL tests add reliability around the DB access, which is
typically one of the most error-prone areas.</li>
<li>Design-wise, this encapsulation usually improves maintainability.</li>
</ol>
<p>Two more subtle benefits:</p>
<ol>
<li>Clearer context makes it easier for the agent to focus:
<ul>
<li>Outside the DAL, the only DB context the agent needs is the narrow DAL interface. Without a DAL, DB access spreads
across many files and patterns, making it harder to surface the relevant details.</li>
<li>Inside the DAL, the agent mostly needs the rest of the interface plus the DB logic contained within the DAL.</li>
</ul>
</li>
<li>Protect the feedback loop from future disasters:
<ul>
<li>As mentioned above, if you don&rsquo;t have a fast-enough DB setup, you might be forced to have a DAL and create a fake (simulator) for it.</li>
<li><strong>BUT</strong>, even if you do have a fast DB setup TODAY - in 6 months you might need to change the DB for some of your
data for scale reasons. And the new DB might not have a fast-enough setup.</li>
<li>So if you don&rsquo;t start with a DAL today, you will be forced to create a new DAL and a fake for it or say goodbye to
your agent&rsquo;s feedback loop.</li>
<li>But by then, you might have thousands of lines of code that access the DB directly, and you&rsquo;ll need to refactor
all that sensitive code into the new DAL - which might be very expensive and introduce bugs.</li>
<li>Don&rsquo;t risk it.</li>
</ul>
</li>
</ol>
<h2 id="what-choices-do-we-need-to-make-for-the-dal-tests">What choices do we need to make for the DAL tests?<a hidden class="anchor" aria-hidden="true" href="#what-choices-do-we-need-to-make-for-the-dal-tests">#</a></h2>
<p>So we want to have a DAL, and we want to have our agent write thorough tests for it.</p>
<p>The setup is similar to the API tests, so the same pitfalls apply:</p>
<ul>
<li>Run everything through the DAL&rsquo;s external interface (the repository API that uses domain models), not the ORM or other
internal APIs.</li>
<li>Keep the tests fast; the DB considerations are similar to the API test setup.</li>
</ul>
<h1 id="summing-up-our-patterns">Summing up our patterns<a hidden class="anchor" aria-hidden="true" href="#summing-up-our-patterns">#</a></h1>
<p>Because API-level tests are the &ldquo;truth&rdquo;, a fast and reliable DB setup makes them enough to give the agent a solid
feedback loop.<br>
In my recent experience, this works so well that I wouldn&rsquo;t start a new service without them.</p>
<p>For a small CRUD, API tests might be sufficient, but I still recommend adding a DAL with thorough tests.<br>
Yes, it&rsquo;s more code, another abstraction, and some overlap, but the agent handles it fine (often better, thanks to
clearer context) and the system ends up sturdier and easier to maintain.</p>
<h1 id="example-project">Example project<a hidden class="anchor" aria-hidden="true" href="#example-project">#</a></h1>
<p>To make this concrete, I

<a href="https://github.com/shaigeva/project_management_crud_example" target="_blank">created a small-ish CRUD backend
project</a>
 that applies the design patterns above.<br>
If you&rsquo;d like to see them in practice, browse the code (links to specific files below).</p>
<p>It&rsquo;s a Python backend for a basic project management app with:</p>
<ul>
<li>Organizations (tenants) with isolated data</li>
<li>Role-based access control (Admin, Project Manager, Write Access, Read Access)</li>
<li>Projects containing tickets with configurable workflows</li>
<li>Epics that span multiple projects</li>
<li>Comments on tickets</li>
<li>Activity logs and audit trails with permission-based access</li>
</ul>
<p>There&rsquo;s a rough UI covering part of the functionality. It&rsquo;s not part of the workflow
I&rsquo;m describing; I just created it to get a feel that the API is usable:</p>
<p><img alt="Example project management app UI" loading="lazy" src="/ai_frameworks/project_management_example_app_gif.gif"></p>
<h2 id="project-structure">Project structure<a hidden class="anchor" aria-hidden="true" href="#project-structure">#</a></h2>
<p>This project was created pretty much entirely by an AI agent (Claude Code, with guidance
of course) - I set up scaffolding and agent rules for coding conventions
and the feedback loop, and the rest was generated by the agent in several sessions.</p>
<p>The stack uses common Python tools such as FastAPI and SQLAlchemy.<br>
It&rsquo;s a local-only SQLite setup with a few thousand lines of code, enough to show the design on something richer than
&ldquo;Hello, World&rdquo;.</p>
<p>The structure mirrors what we&rsquo;ve discussed. Examples:</p>
<ul>
<li>One of the API routers: <a href="https://github.com/shaigeva/project_management_crud_example/blob/main/project_management_crud_example/routers/ticket_api.py" target="_blank" rel="noopener noreferrer">https://github.com/shaigeva/project_management_crud_example/blob/main/project_management_crud_example/routers/ticket_api.py</a></li>
<li>DAL: <a href="https://github.com/shaigeva/project_management_crud_example/blob/main/project_management_crud_example/dal/sqlite/repository.py" target="_blank" rel="noopener noreferrer">https://github.com/shaigeva/project_management_crud_example/blob/main/project_management_crud_example/dal/sqlite/repository.py</a></li>
<li>Tests:
<ul>
<li>One of the API tests: <a href="https://github.com/shaigeva/project_management_crud_example/blob/main/tests/api/test_ticket_api.py" target="_blank" rel="noopener noreferrer">https://github.com/shaigeva/project_management_crud_example/blob/main/tests/api/test_ticket_api.py</a></li>
<li>One of the DAL tests: <a href="https://github.com/shaigeva/project_management_crud_example/blob/main/tests/dal/test_ticket_repository.py" target="_blank" rel="noopener noreferrer">https://github.com/shaigeva/project_management_crud_example/blob/main/tests/dal/test_ticket_repository.py</a></li>
<li>In addition to the API and DAL tests, there are some unit tests for specific modules,
in cases that a module can be tested in isolation without mocks etc.</li>
</ul>
</li>
</ul>
<h2 id="tests-some-extra-details">Tests: some extra details<a hidden class="anchor" aria-hidden="true" href="#tests-some-extra-details">#</a></h2>
<p>Spinning up a fresh DB is quick here, so each test gets a new physical SQLite file in a temp directory. We seed it with
test data and discard it when the test finishes.</p>
<p>The full suite takes ~20 seconds.<br>
That&rsquo;s fine for an example, but in a larger project I&rsquo;d default to in-memory runs, cache more of the setup, and have the
agent run only the tests related to the change so typical runs stay in the several-second range.</p>
<h2 id="honorary-mention-spec-driven-development">Honorary mention: Spec-driven development<a hidden class="anchor" aria-hidden="true" href="#honorary-mention-spec-driven-development">#</a></h2>
<p>The focus of this post is on code &amp; tests design, so I won&rsquo;t get into the details here, but it&rsquo;s worth a mention that
workflow-wise, the project is spec-driven in spirit.<br>
It follows a ~consistent pattern to move from high-level spec to low-level implementation, and this has been a major part
in getting the agent to do consistent, focused work.</p>
<pre tabindex="0"><code>  ┌──────────────────────────────────┐
  │        High-Level Spec           │  (main features)
  └────────────────┬─────────────────┘
                   │
                   ▼
  ┌──────────────────────────────────┐
  │        Detailed Specs            │  (persistent - requirements &amp; acceptance
  └────────────────┬─────────────────┘   criteria for specific features)
                   │
                   ▼
  ┌──────────────────────────────────┐
  │  Current Feature Implementation  │  (ephemeral - archived when done)
  │  Plan                            │
  └────────────────┬─────────────────┘
                   │
                   ▼
  ┌──────────────────────────────────┐
  │        Code and Tests            │
  └──────────────────────────────────┘
</code></pre><h1 id="summing-up">Summing up<a hidden class="anchor" aria-hidden="true" href="#summing-up">#</a></h1>
<p>My goal with this post was to highlight foundational design and testing patterns for API services and show why they are
better than others for AI-first development.</p>
<p>I shared the details to keep things concrete enough to try.<br>
In my experiments, these patterns perform very well, and I hope
they&rsquo;re useful to you, too.</p>
<p>I&rsquo;d love to hear your thoughts on social
(<a href="https://x.com/shai_ge" target="_blank" rel="noopener noreferrer">twitter / x</a>, <a href="https://www.linkedin.com/in/shai-geva-bb51404/" target="_blank" rel="noopener noreferrer">linkedin</a>)!</p>
<hr>

<div style="text-align: center; display: block; width: 100%;">
<a href="/posts/ai_frameworks/02_ai_feedback_loop_example">&lt;&lt; previous post: What an AI Feedback Loop Looks Like</a>
|
next post: (coming soon) &gt;&gt;
</div>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://shaigeva.com/tags/ai/">AI</a></li>
      <li><a href="https://shaigeva.com/tags/ai-first/">AI-First</a></li>
      <li><a href="https://shaigeva.com/tags/ai-driven/">AI-Driven</a></li>
      <li><a href="https://shaigeva.com/tags/ai-first-design-patterns/">AI-First Design-Patterns</a></li>
      <li><a href="https://shaigeva.com/tags/design-patterns/">Design-Patterns</a></li>
      <li><a href="https://shaigeva.com/tags/testing/">Testing</a></li>
      <li><a href="https://shaigeva.com/tags/backend/">Backend</a></li>
      <li><a href="https://shaigeva.com/tags/api/">API</a></li>
      <li><a href="https://shaigeva.com/tags/dal/">DAL</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://shaigeva.com/">Shai Geva</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
