[{"content":"Footgun 10: wrong priorities\nWe saw a bunch of different practices, and how they will affect us by changing the properties of our tests.\nThe bug funnel is all about performance. “Testing implementation instead of behavior” us about maintainability and strength.\nBut how do we prioritize?\nNow, the objective of tests is their strength. We have tests so that they catch bugs.\nThe unintuitive thing is that this is not what we should prioritize when we work.\nStart with making them maintainable, Then make sure they are fast enough And then make them strong\nHere’s the thing\nSlow tests are weak, or at least they are EVENTUALLY weak.\nLet’s say that, as a team, we decided that we are not willing to have tests that run for more than 30 minutes.\nIf, at some point the tests reach 30 minutes…\nIt becomes very difficult to add more tests.\nSo after enough time, there will be a lot of code that’s not tested well.\nAnd the same thing happens with maintenance. It’s more subtle, but if tests are not maintainable, it costs more to have them, and we end up creating fewer tests. So again, they will be eventually weak.\nAnd, maintainability issues can also make it difficult to handle performance. An example we saw is test isolation and parallelization.\nIn other words, Maintainability is a necessary condition for performance, and both are necessary conditions for strength.\nSo make maintainability the priority. Testing a single fact, code design and all the others.\nWhen you have a choice to make - I suggest to go with the most maintainable option almost always. Even at the cost of other things. Because in the long run, that’s how we get tests that let us move fast, and have confidence in our code.\nConclusion ","permalink":"https://shaigeva.com/posts/10_footguns/10_wrong_priorities/","summary":"\u003cp\u003eFootgun 10: wrong priorities\u003c/p\u003e\n\u003cp\u003eWe saw a bunch of different practices,\nand how they will affect us by changing the properties of our tests.\u003c/p\u003e\n\u003cp\u003eThe bug funnel is all about performance.\n“Testing implementation instead of behavior” us about maintainability and strength.\u003c/p\u003e\n\u003cp\u003eBut how do we prioritize?\u003c/p\u003e\n\u003cp\u003eNow, the objective of tests is their strength.\nWe have tests so that they catch bugs.\u003c/p\u003e\n\u003cp\u003eThe unintuitive thing is that this is not what we should prioritize when we work.\u003c/p\u003e","title":"Footgun #10 - Wrong Priorities"},{"content":"Footgun 9: Slow tests\nYeah, slow tests are not fun.\nI’ll talk about two ways in which they are not fun.\nThe first way is what I like to think of as the bottleneck and the time bomb.\nThe bottleneck here is where the tests take so long to run, that we have a long queue of tasks waiting to be merged to the main branch.\nWhat kind of numbers are we’re talking about here?\nAssume we have, say, 10 work-hours each day.\nwith tests that take 5 minutes.\nSo, that’s 12 merges per hour,\nWhich is 120 merges a day before the tests slow us down.\nFor most teams, that virtually never happens, so a 5-minute test suite -\nnot a bottleneck.\nOn the other extreme, and it usually won’t get to that but just so it’s easy to imagine -\nif the test suite takes 2 HOURS,\nThen we can only merge 5 tasks to main each day.\nWhenever we want to wrap up a bunch of tasks quickly, maybe before a major version, the merge queue length becomes days. If the tests sometimes fail, then this can happen on any random day.\nIt just doesn’t work.\nThe team will probably just stop waiting for the tests to pass before merging, and spend a lot of time with the tests being broken.\nNow, we can SURVIVE this way. But it’s a lot of extra work and it’s really not what we want.\nAnd really, even with less extreme numbers,\nFrom what I see:\nWith a 30 minutes test suite\nThe same things happen. They happen less, but they happen.\nAnd, a few years ago I was actually part of a team where this happened.\nWhen the tests took 20 minutes, I understood it’s a time bomb and eventually things were going to get bad.\nBut I didn’t have this clear phrasing of exactly how the slowness would be a problem. The bottleneck.\nAnother problem there was that the tests were also flaky and we always needed to fix them, so it wasn’t clear to most people that slowness was the more urgent problem.\nAfter a while, we were getting all these problems every few weeks. Multi-day merge queues, everything was stuck. Real crisis mode.\nIt only became ok after we did an expensive project and made the tests run in parallel. Tests would still break sometimes, but the queue got back to zero fast enough so it was not a crisis.\nThe question is - what do we do about this?\nWe don’t want premature optimization, so what we need on day 1 is to make sure that WHEN we want to optimize, it’s not going to be a very expensive project.\nAnd specifically, it should be possible to run the tests in parallel because that’s going to be the go-to solution.\nThe only thing we need for that, is to remember the footgun about isolated tests. If the tests don’t affect each other, they can run in parallel.\nMy advice is to consider this as a must-have.\nAnother way that slow tests can hurt us is by making our feedback loop longer.\nThe feedback loop is how fast we learn about bugs and understand what happened. And I’m talking about any type of bug here - anything from a typo to complex concurrency issues.\nThe feedback loop is very important, And anything that makes it shorter is very good. Even a squiggly red line in the IDE.\nI usually aim for a setup where most of the time, I’m working in watch-mode, so the tests re-run every time a file changes, and I run a sub-set of the tests that finishes within 2 or 3 seconds.\nBeing on the fast side is great. For example, if a test fails just a few seconds after I wrote the code - I instantly understand what’s going on.\nWith a 10-minute tests suite in CI - the commit with a failing test contains a lot of code. Plus my brain will do a context switch and go catch up on slack. So when I try to understand what’s going on with the failing test - it’s a lot more work.\nNow, some tests HAVE to be slow. But, we can still have pretty fast feedback loop.\nWhat helps me here is that instead of asking “How long does it take for the tests to run” I’m asking “How long does it take to catch a bug”\nAnd I’m visualizing this using the “bug funnel”.\nAll possible theoretical bugs come in, and some of them get filtered out on every stage.\nAnd the key observation here is that what matters to the feedback loop is that we catch MOST bugs quickly. We will have a good experience if the feedback loop is USUALLY fast.\nLet’s say we start out with a bug funnel that looks like this. We only have long-running integration tests, and we only run them in CI.\nSo if we create 10 bugs - we need to wait and debug the CI 10 times.\nIf we start adding fast unit tests, then pretty quickly the bug funnel will look more like this. We don’t wait 10 times for the long-running CI anymore. Only for, say, 2 of the bugs.\nFor the rest of the bugs, so most of the time - we’ll have a much faster feedback loop.\nAnd try to run at least some of the tests in watch-mode! You will have a 2-second feedback loop, even if it’s not for everything.\nAnd, as we discussed - you can also use test doubles, that’s why they exist.\nBy the way, local recordings have a very good tradeoff here - you should try it.\nConclusion ","permalink":"https://shaigeva.com/posts/10_footguns/09_slow_tests/","summary":"\u003cp\u003eFootgun 9: Slow tests\u003c/p\u003e\n\u003cp\u003eYeah, slow tests are not fun.\u003c/p\u003e\n\u003cp\u003eI’ll talk about two ways in which they are not fun.\u003c/p\u003e\n\u003cp\u003eThe first way is what I like to think of as the bottleneck and the time bomb.\u003c/p\u003e\n\u003cp\u003eThe bottleneck here is where the tests take so long to run, that we have a long queue of tasks waiting to be merged to the main branch.\u003c/p\u003e\n\u003cp\u003eWhat kind of numbers are we’re talking about here?\u003c/p\u003e","title":"Footgun #9 - Slow Tests"},{"content":"Footgun 8 - test doubles everywhere Sometimes, in a test, we switch a part of the system, a dependency, with an alternative implementation.\nThese are called test doubles. Things like stubs, mocks and fakes.\nThe main reason we use them is performance - if the real thing is too slow to run a lot of tests, we switch it with a fast test double.\nTest doubles can be useful, but…\nTest doubles are a re-implementation. They know the implementation details of the thing they’re replacing. Different types of test doubles do it differently, but this is what they do.\nThe main problem this causes is correctness. The test double might not behave exactly like the real thing, and that makes the tests less accurate, less correct.\nAnd as times goes by,\nthe real thing might be slowly changed,\nbut the test double would stay the same, so it would drift further and further from reality.\nAnd, of course, this can hurt your foot.\nThis is actually a flavor of the implementation vs. behavior problem.\nThere are some differences, but essentially, it’s the same category of issues - tests that use test doubles are not as good at catching bugs, and sometimes they fail even though the code is correct, causing all that extra work.\nSo, test doubles - use, with caution.\nThe question is - how do we avoid the pitfalls? And I’ll suggest a couple of ideas\nFirst - code design. So important.\nTry to design so you can test a lot of functionality effectively, with fast unit tests, that don’t need test doubles. Not ALWAYS possible, but a lot of times it is.\nAnother thing is to choose which test double you’re working with. And I suggest to mostly use fakes.\nA fake behaves like your dependency, but fast.\nFor example, a fake database table can be an in-memory list of tuples, where each tuple is a row. In tests - it behaves the same way.\nWe can make a fake more realiable by writing some tests, not for the code - but for the fake itself.\nFor example, we can run the same operations against the fake and the real thing and verify we get the same results.\nIt’ll never be 100% the same - we make tradeoffs in how much we are willing to invest in testing the fake.\nSometimes, a reliable fake already exists. For example, if you’re using SQLite - Python actually has a built-in, in-memory implementation.\nSo google it, maybe you’ll get lucky.\nAn interesting thing you can do with fakes, is to run exactly the same test - once with a fake, and once with the real thing.\nFor example, maybe we have 10 tests, and that’s too much to run against the real thing.\nSo we run all 10 with the fake.\nAnd then, we choose the 2 most important ones, and we run them ALSO with the real thing. And this gives us some real world certainty.\nThe essence of the idea is to use test doubles, but selectively verify their correctness until we get an acceptable tradeoff.\nAnother way to “use test doubles and verify” is by caching recordings. We can record HTTP requests, DB actions, or anything else.\nFor example, at CodiumAI, We have an HTTP service that calls another HTTP service - an AI layer that does code analysis and generation.\nSo, in our tests, we record and save the HTTP interactions between the main server and the AI server.\nLocally, we run the tests against the recordings, so they are very fast.\nBut - we also verify. In the cloud, we also run the tests against the real thing to make sure they are still valid.\nConclusion ","permalink":"https://shaigeva.com/posts/10_footguns/08_test_doubles_everywhere/","summary":"\u003cp\u003eFootgun 8 - test doubles everywhere\nSometimes, in a test, we switch a part of the system, a dependency, with an alternative implementation.\u003c/p\u003e\n\u003cp\u003eThese are called test doubles. Things like stubs, mocks and fakes.\u003c/p\u003e\n\u003cp\u003eThe main reason we use them is performance - if the real thing is too slow to run a lot of tests, we switch it with a fast test double.\u003c/p\u003e\n\u003cp\u003eTest doubles can be useful, but…\u003c/p\u003e","title":"Footgun #8 - Test Doubles Everywhere"},{"content":"Let’s say our Book Store is a web service, and it uses a DB.\nWe’ll think about two alternative test suites - “behavior tests” and “implementation tests” And try to imagine what our life will look like if we would have chosen one test suite or the other.\nWe’ll look at an almost identical test in both test suites.\nThe test verifies that if we edit the description of a book, then it has really been updated. Pretty simple.\nBoth tests have the same flow - Create a book Edit the book Get the updated description Make the assertion.\nThe behavior test does everything through the external http API, IN THE SAME WAY things would be done in the actual system.\nThe implementation test does some of the things at a lower level: It creates the book by directly creating a record in the database, and it also checks the updated description through the DB.\nSo the behavior test only looks at the WHAT - It looks at things as they appear from outside.\nThe implementation test also knows about HOW. It knows how the code will change the DB.\nNow, checking the implementation like this will USUALLY be equivalent to the behavior - but not always.\nBut why does this matter to us?\nLet’s look at a possible scenario: We’ve had this test suite for a while, maybe even years. We’ve invested a lot in them, and we rely on them.\nNow, we’re making a change to optimize the database.\nWe’re moving the description out of the Book table, and into a separate table.\nHowever, we’re not deleting the old field yet - we’ll do that later after all the data has moved to the new table.\nNow, we’re finished with everything else, and, it’s time to update the edit-book endpoint.\nNow, what if we just forgot to update the edit-book endpoint? Completely forgot.\nIt now changes the wrong field in the database so behavior-wise, it doesn’t do anything. If this gets to production, then we created a major bug.\nIf we chose behavior tests - The test only uses the external API. The behavior test…\nDoes not care about implementation details. So if the behavior is wrong, the test will fail, just like it should.\nThe regression bug was prevented. Everything’s ok.\nIf we chose the implementation test -\nThe test looks directly at old description field in the DB. When we run the test, the old description field will change, just like before, so the test will not fail.\nThe regression bug was not prevented. And a major bug made it to production.\nIt’s not ok.\nOn the other side of this, what if we made the change correctly? Edit book now changes the new table instead of the old field. No bugs, everything’s fine.\nIf we chose the behavior test - Everything behaves correctly, so the test will pass. We don’t need to do anything.\nIf we chose the implementation test - The old field is not updated any more, so even though the code is correct, this test will fail.\nThe distinction here is that the failure reason is not that the code is not correct. The test fails because it has become technically invalid.\nSo, we have extra work - we need to figure out whether the failure is real or technical. And then we’ll need to update the test. Also - because we changed the test, we now have less confidence in it. We need to learn to trust it again.\nOn large code bases, this can become a real pain. You have to update the tests, even if the code change has no bugs, and sometimes even if the test has nothing to do with the feature you worked on.\nYou end up wasting hours and you hate the test suite.\nSumming up\nCohesive, behavior tests - are closer to reality\nThey are better at protecting us. The create less redundant work And we have higher confidence in them in the long run.\nOne more thing worth mentioning: we looked at an example of a small, incremental change. But sometimes, we need to make BIG changes. SCARY changes. It happens less often but when it happens it’s a big deal.\nFor example, in a lot of companies, at some point, the DB doesn’t deal with the scale well. We have stability issues, and we need to make a big change - maybe move the data to a different type of database. And that’s when tests are MOST important.\nAnd if we went with behavior level tests - everything will be fine. Those same tests that we’ve been running with for 3 years now - we don’t change them. When they pass, we’re done.\nBut if we went with Implementation level tests - they all become technically invalid and they all fail. We will need to port all of them to use the new database, but more importantly: because we’re changing them - we’re not going to trust them enough.\nThis might make the difference between a project that takes a few weeks, and a company-level event that drags out for months while the product has stability issues.\nSo I cannot recommend enough. Test behavior. A cohesive whole.\nConclusion a\n","permalink":"https://shaigeva.com/posts/10_footguns/07_improper_test_scope/","summary":"\u003cp\u003eLet’s say our Book Store is a web service, and it uses a DB.\u003c/p\u003e\n\u003cp\u003eWe’ll think about two alternative test suites - “behavior tests” and “implementation tests”\nAnd try to imagine what our life will look like if we would have chosen one test suite or the other.\u003c/p\u003e\n\u003cp\u003eWe’ll look at an almost identical test in both test suites.\u003c/p\u003e\n\u003cp\u003eThe test verifies that if we edit the description of a book, then it has really been updated.\nPretty simple.\u003c/p\u003e","title":"Footgun #7 - Improper Test Scope"},{"content":" (this mini-post is part of a series about good testing practices)\nJust like with product code, if we put too many things in the same place we get a mess.\nMy rule of thumb is to try hard to test a single fact about the behavior of the code. And it helps if I use these specific words mentally.\nSINGLE. FACT. About the BEHAVIOR.\nExample Let\u0026rsquo;s continue with our example from the previous post, and say we have a book store and we\u0026rsquo;re testing the edit book functionality.\nFor example, that\u0026rsquo;s a single fact about the behavior the code: test_user_can_edit_their_own_book And, this is not a single fact, it\u0026rsquo;s too general: test_edit_book How do they compare? Easy to understand? Single fact test: It\u0026rsquo;s clear what the test checks. It\u0026rsquo;s clear that it only checks that. General test: we\u0026rsquo;ll need to read and understand all the test code to know. Easy to debug? Single fact test: If it fails, it\u0026rsquo;s clear what functionality stopped working. And because it\u0026rsquo;s small, it\u0026rsquo;ll be easy to debug it. General test: if it fails, anything related to edit book might have failed. We\u0026rsquo;ll need to dig in. And it does a lot of things, so debugging might be a lot of work. Conclusion Try to have each test case will test a SINGLE FACT about the BEHAVIOR of the code.\nThis makes a huge difference and it\u0026rsquo;s worth it to invest a lot into this.\n\u0026lt;\u0026lt; previous post: Unclear Language | next post: Improper Test Scope \u0026gt;\u0026gt; ","permalink":"https://shaigeva.com/posts/10_footguns/06_testing_too_many_things/","summary":"\u003cstyle\u003e\n.code-example {\n  background-color: #2E2E33;\n  padding: 10px;\n  margin-bottom:10px;\n  border-radius: 5px;\n  font-family: monospace;\n  white-space: pre;\n  color: #d5d5d6;\n  font-size: .78em;\n  line-height: 1.5;\n}\n.highlight-red {\n  color: red;\n}\n.highlight-green {\n  color: green;\n}\n\u003c/style\u003e\n\n\u003cp\u003e(this mini-post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eJust like with product code, if we put too many things in the same place we get a mess.\u003c/p\u003e\n\u003cp\u003eMy rule of thumb is to try hard to test a single fact about the behavior of the code.\nAnd it helps if I use these specific words mentally.\u003c/p\u003e","title":"Footgun #6 - Testing Too Many Things"},{"content":"This is a series of posts, following a talk I gave (twice - at Pycon-US 2023 and Pycon-IL 2024), about testing best (and not-so-best) practices.\nThe talk shares 10 practices that I had bad experience with, along with ways of avoiding them.\nStarting with simple (but useful!), and moving on to more complex ideas:\nThere are no tests (warm up) Untested tests The tests are not isolated No locality of behavior Unclear language Testing too many things Improper test scope Test doubles everywhere Slow tests Wrong priorities Videos and slide decks from the talks\n10 Ways To Shoot Yourself In The Foot With Tests - PyCon-US 2023 (English) Given at Salt Lake City, Utah.\nSlides: Slideshare PyCon US 2023\n10 Ways To Shoot Yourself In The Foot With Tests - PyCon-IL 2024 (Hebrew) Slides: Slideshare PyCon IL 2024\n","permalink":"https://shaigeva.com/posts/10_footguns/ten_footguns/","summary":"\u003cp\u003eThis is a series of posts, following a talk I gave (twice - at Pycon-US 2023 and Pycon-IL 2024), about testing best (and not-so-best) practices.\u003c/p\u003e\n\u003cp\u003eThe talk shares 10 practices that I had bad experience with, along with ways of avoiding them.\u003c/p\u003e\n\u003cp\u003eStarting with simple (but useful!), and moving on to more complex ideas:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/01_there_are_no_tests/\"\u003eThere are no tests\u003c/a\u003e (warm up)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/02_untested_tests/\"\u003eUntested tests\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/03_the_tests_are_not_isolated/\"\u003eThe tests are not isolated\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/04_no_locality_of_behavior/\"\u003eNo locality of behavior\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/05_unclear_language/\"\u003eUnclear language\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTesting too many things\u003c/li\u003e\n\u003cli\u003eImproper test scope\u003c/li\u003e\n\u003cli\u003eTest doubles everywhere\u003c/li\u003e\n\u003cli\u003eSlow tests\u003c/li\u003e\n\u003cli\u003eWrong priorities\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eVideos and slide decks from the talks\u003c/p\u003e","title":"10 Ways To Shoot Yourself In The Foot With Tests"},{"content":" (this mini-post is part of a series about good testing practices)\nAnother problem that makes it more difficult to understand tests is unclear language.\nTwo guidelines that help me deal with this:\nWe want to use decisive language We want the language to be specific and explicit Suppose we have a book store and we\u0026rsquo;re testing the functionality for editing a book.\nLet\u0026rsquo;s see some examples of test phrasing:\ndef test_edit_book(): ... This is simply too general.\nThere are so many things that might be tested, and this means almost nothing about what will get tested in practice.\ndef test_edit_book_works_correctly(): ... Adding things like \u0026ldquo;it works\u0026rdquo; or \u0026ldquo;it\u0026rsquo;s correct\u0026rdquo; - most of the time, this is just bloat.\nIt only makes the name bigger, but doesn\u0026rsquo;t give us any extra information.\ndef test_user_should_be_able_to_edit_their_own_book(): ... That\u0026rsquo;s much better - it\u0026rsquo;s a lot more specific.\nThe only problem here is the indecisive language.\nWhy \u0026ldquo;should\u0026rdquo;?\nWill this ever NOT be correct?\nIt\u0026rsquo;s both bloated and confusing.\nSo this as well - not optimal.\ndef test_user_can_edit_their_own_book(): ... That\u0026rsquo;s much better in my opinion.\nIt\u0026rsquo;s decisive, explicit and specific.\nI suggest to aim towards this whenever possible.\nConclusion When phrasing test names and descriptions, try to aim for decisive, specific and explicit language.\n","permalink":"https://shaigeva.com/posts/10_footguns/05_unclear_language/","summary":"\u003cstyle\u003e\n.code-example {\n  background-color: #2E2E33;\n  padding: 10px;\n  margin-bottom:10px;\n  border-radius: 5px;\n  font-family: monospace;\n  white-space: pre;\n  color: #d5d5d6;\n  font-size: .78em;\n  line-height: 1.5;\n}\n.highlight-red {\n  color: red;\n}\n.highlight-green {\n  color: green;\n}\n\u003c/style\u003e\n\n\u003cp\u003e(this mini-post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eAnother problem that makes it more difficult to understand tests is unclear language.\u003c/p\u003e\n\u003cp\u003eTwo guidelines that help me deal with this:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWe want to use decisive language\u003c/li\u003e\n\u003cli\u003eWe want the language to be specific and explicit\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSuppose we have a book store and we\u0026rsquo;re testing the functionality for editing a book.\u003cbr\u003e\nLet\u0026rsquo;s see some examples of test phrasing:\u003c/p\u003e","title":"Footgun #5 - Unclear Language"},{"content":"(this mini-post is part of a series about good testing practices)\nOne testing problem that doesn\u0026rsquo;t get enough attention in my opinion is tests that don\u0026rsquo;t have locality of behavior.\nBy that I mean cases where a test is broken down into different parts in a way that makes understanding more difficult.\nThis is important in every type of code, and tests are no exception.\nExample: non-local data Consider this test:\ndef test_something(): data = Path(PATH_TO_DATA_FILE).read_text() assert calc_something(data) == 4.5 The data that the test uses is in a different file, so in order to understand the test we will need to locate that file and open it.\nEven if the data was in the same file, but a different place - it would still be an issue.\nNow, sometimes we don\u0026rsquo;t have a choice, and it\u0026rsquo;s the only way to do it.\nBut sometimes we do.\nFor example, if we can find a data example that\u0026rsquo;s small enough, we can do something like this:\ndef test_something(): data = “”” { \u0026lt;JSON data\u0026gt; } “”” assert calc_something(data) == 4.5 This is exactly the same test, but the data is local so it\u0026rsquo;s going to be much easier to understand at a glance, without \u0026ldquo;breaking the flow\u0026rdquo;.\nIt\u0026rsquo;s easier in tests than in production code One of the main problems with achieving locality of behavior is that it conflicts with DRY (\u0026ldquo;don\u0026rsquo;t repeat yourself\u0026rdquo;).\nWhat\u0026rsquo;s the problem with code duplication in production code?\nYou\u0026rsquo;ll often hear people talking about the \u0026ldquo;economics\u0026rdquo; - if you repeat a piece of code 3 times, then if you need to change that logic, you would need to do that work 3 times.\nHowever, this is actually a secondary consideration, especially if the number of repetitions is not high (let\u0026rsquo;s say 5 or less).\nThe real issue with repeating yourself is that duplication is an implicit dependency.\nIf you repeat the same logic in 3 different places, there\u0026rsquo;s a risk that if the logic needs to change, you would not notice one of these places, which would cause that \u0026ldquo;usage\u0026rdquo; to be deprecated and incosistent with the rest of the code - which will result in bugs and maintainability overhead, of course.\nThe nice thing about tests here is that this consideration is weaker, because of several factors.\nWhen we change code that has a test and make it behave differently, the relevant tests will usually break, so we will have something that points out to the duplicated \u0026ldquo;usages\u0026rdquo;. Where in production code - we only get that benefit if that piece of code has relevant tests - which might be very far from \u0026ldquo;always\u0026rdquo;. If the tests are focused and verify only a single fact, the number of times that we have duplication will be lower. And it\u0026rsquo;s far easier to write a test (at least a test that\u0026rsquo;s not end-to-end) that \u0026ldquo;checks one thing\u0026rdquo; than it is to write code that \u0026ldquo;does a single thing\u0026rdquo;, because if a piece of code does multiple things - you can just write multiple tests that run it separately, and each of them would test one thing. And, finally, if the mistake does happen in test-code and we forget to update a \u0026ldquo;usage\u0026rdquo; - it\u0026rsquo;ll render the test \u0026ldquo;wrong\u0026rdquo;, but it won\u0026rsquo;t actually cause a bug. Making a test wrong is bad, but it\u0026rsquo;s not as bad as a production bug, unless it affects a lot of tests. Of course, duplication is still something to consider - if there\u0026rsquo;s complex setup, or something that\u0026rsquo;s not complex but repeats many times - it\u0026rsquo;s probably worth it to extract functionality.\nThe point is that in production code, the right time to extract functionality is often after two or three usages - but in tests the balance allows for more.\nConclusion Be aware that tests which are \u0026ldquo;non-local\u0026rdquo; are far more difficult to maintain, and make a conscious effort to find ways to reduce the problem.\n","permalink":"https://shaigeva.com/posts/10_footguns/04_no_locality_of_behavior/","summary":"\u003cp\u003e(this mini-post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eOne testing problem that doesn\u0026rsquo;t get enough attention in my opinion is tests that don\u0026rsquo;t have locality of behavior.\u003c/p\u003e\n\u003cp\u003eBy that I mean cases where a test is broken down into different parts in a way that makes understanding more difficult.\u003c/p\u003e\n\u003cp\u003eThis is important in every type of code, and tests are no exception.\u003c/p\u003e\n\u003ch2 id=\"example-non-local-data\"\u003eExample: non-local data\u003c/h2\u003e\n\u003cp\u003eConsider this test:\u003c/p\u003e","title":"Footgun #4 - No Locality of Behavior"},{"content":"(this post is part of a series about good testing practices)\nWriting tests that are not isolated is a sure way to create unnecessary work for ourselves.\nBy \u0026ldquo;tests that are not isolated\u0026rdquo;, I mean tests that sometimes have a different outcome (failing / passing) if we run only a subset of them, if we run them in a different order or if we run them in parallel.\nWhy is this a problem? Let\u0026rsquo;s say we have 30 tests, and test 24 passes if we run it individually but fails if we run the entire test suite.\nIf the test was isolated, then there are only a limited number of things that can go wrong. The things that the test actually executes will almost certainly be the cause of the failure.\nBut now, of course, the reason for failure is not what test 24 checks, and not what any of the other tests checks either. The failure is caused by an implicit interaction between test 24 and, say, test 8.\nThis is difficult to debug, of course, because we would first need to play detective and find that it\u0026rsquo;s test 8 that\u0026rsquo;s causing the problem (which might take a LOT of effort, especially if the tests are e2e tests that run slowly or only in CI), and then track the combined flow of the tests to find the problematic interaction.\nBut it\u0026rsquo;s worse than this - what often happens is that we run the entire test suite, either locally or in CI, and test 24 fails - so we start analyzing it as if test 24 fails individually. We don\u0026rsquo;t even consider that it\u0026rsquo;s a combined failure, and sometimes we waste hours chasing irrelevant clues, just to get to the point that we notice the test passes if its executed separately.\nSo the fact that we have \u0026ldquo;individual tests\u0026rdquo; is actually a misleading illusion that only distracts us from the unfortunate truth - we have one gigantic test with inter-connected sub-sections.\nIf e2e tests run in parallel and sometimes run in different order, which makes this cross-test-failure flaky, we get the perfect storm and we might spend days of work on this.\nAnd, lastly, the situation could actually be worse - it might be that test 24 SHOULD fail, but it passes because of test 8, hiding a bug.\nWhat are the causes behind this? There are rare cases like technical limitations (e.g. external service rate limits), but in the vast majority of cases - it\u0026rsquo;s shared mutable state.\nA typical scenario for unit tests would be a global in-memory object, and for end-to-end tests maybe a row in a database.\nTest 8 would change that shared thing, and test 24 would therefore have a different state when it starts to execute, leading to the failure.\nSharing a resource that\u0026rsquo;s immutable (or never changes in practice) is not a problem - if it never changes, then it\u0026rsquo;s the same whether or not it\u0026rsquo;s accessed by multiple \u0026ldquo;consumers\u0026rdquo;.\nI\u0026rsquo;ve also seen more subtle cases of shared mutable state, for example tests that would rely on an email being sent through an actual email-sending service, and multiple tests checked for the same email.\nWhat can we do? First of all, because in my experience this is so often a very painful problem - I suggest to avoid this from the start, even if it means extra work or delaying writing some tests if can\u0026rsquo;t make them isolated at the moment.\nDO NOT create a suite of non-isolated e2e tests that use long-living mutating database entities. This is very very likely to cause a crisis down the road.\nBecause the problem is usually shared mutable state, we mostly just need to avoid that specific issue.\nUnit tests can use the same in-memory objects, but they have to be immutable, or at least never changed by convention.\nIf this is not an option - either create the object in every test or have some original which you clone at the beginning of the tests.\nThe trickier part are higher-level tests that use expensive resources like a database, where full initializations might cause the tests to be too slow.\nIf it\u0026rsquo;s not too expensive - initialize the entire thing, of course.\nOtherwise - make sure to clean up - delete created resources and revert the database / file system / etc. to its original state before continuing.\n","permalink":"https://shaigeva.com/posts/10_footguns/03_the_tests_are_not_isolated/","summary":"\u003cp\u003e(this post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eWriting tests that are not isolated is a sure way to create unnecessary work for ourselves.\u003c/p\u003e\n\u003cp\u003eBy \u0026ldquo;tests that are not isolated\u0026rdquo;, I mean tests that sometimes have a different outcome (failing / passing) if we run\nonly a subset of them, if we run them in a different order or if we run them in parallel.\u003c/p\u003e\n\u003ch2 id=\"why-is-this-a-problem\"\u003eWhy is this a problem?\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s say we have 30 tests, and test 24 passes if we run it individually but fails if we run the entire test suite.\u003c/p\u003e","title":"Footgun #3 - The Tests Are Not Isolated"},{"content":"(this post is part of a series about good testing practices)\nSometimes our tests lie to us.\nWe have a test that was supposed to protect us from some bug, but that bug happened after all.\nOf course, what happened was that we made a mistake, and the test didn\u0026rsquo;t really verify what we thought it does.\nAs it turns out - when we write a test, it\u0026rsquo;s a good idea to spend a little effort to verify the test actually works.\nTo make sure that if the bug happens, the test does indeed fail.\nHow to avoid this My suggestion -\nWhen you write a test, for every assertion you write, make a small change:\nEither change the code a little and introduce the bug. Or change the test a little bit, so it verifies something a little different. This way you will be able to see if the test would have failed in the way you expect it to, and you\u0026rsquo;ll be able to count on it.\n","permalink":"https://shaigeva.com/posts/10_footguns/02_untested_tests/","summary":"\u003cp\u003e(this post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eSometimes our tests lie to us.\u003c/p\u003e\n\u003cp\u003eWe have a test that was supposed to protect us from some bug, but that bug happened after all.\u003c/p\u003e\n\u003cp\u003eOf course, what happened was that we made a mistake, and the test didn\u0026rsquo;t really verify what we thought it does.\u003c/p\u003e\n\u003cp\u003eAs it turns out - when we write a test, it\u0026rsquo;s a good idea to spend a little effort to verify the test actually works.\u003cbr\u003e\nTo make sure that if the bug happens, the test does indeed fail.\u003c/p\u003e","title":"Footgun #2 - Untested Tests"},{"content":" My name is Shai Geva.\nI\u0026rsquo;ve been creating software for humans, with humans, for over 20 years.\nI\u0026rsquo;m mostly a builder, but have also played more \u0026ldquo;high level roles\u0026rdquo;, like product and management (my current position is tech-lead at Sayata).\nI write about various topics, focusing on code quality and automated tests, which are a great passion for me.\nI also speak at confrences, so far exclusively about tests.\nIf you\u0026rsquo;d like to get notified or talk to me, please follow / ping me on twitter / x or linkedin.\n","permalink":"https://shaigeva.com/about/","summary":"\u003cdiv style=\"position: relative; display: inline-block;\"\u003e\n  \u003cimg src=\"/shai_mic_cover.jpg\" alt=\"Profile pic\" title=\"Profile pic\" style=\"margin:0; display: block; width: 100%;\" /\u003e\n  \u003cdiv style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; background: linear-gradient(to bottom, rgba(0, 0, 0, 0.1) 70%, rgba(0, 0, 0, 0.7) 100%); pointer-events: none;\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eMy name is Shai Geva.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve been creating software for humans, with humans, for over 20 years.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m mostly a builder, but have also played more \u0026ldquo;high level roles\u0026rdquo;, like product and management (my current position is\ntech-lead at \u003ca href=\"https://www.sayata.com/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eSayata\u003c/a\u003e).\u003c/p\u003e","title":"About"},{"content":"(this post is part of a series about good testing practices)\nThis is a \u0026ldquo;warm-up footgun\u0026rdquo; to the blog post series.\nThe easiest way to shoot yourself in the foot, testing-wise, is to have no tests at all.\nIn my experience, writing any tests often helps us - even if these tests are not well-written, and even if they\u0026rsquo;re just a drop in the sea.\nThere are a few reasons I noticed, why moving from no tests at all to even one test for some area of the code is useful.\nCode changes around hotspots Code changes are not uniformly distributed.\nIf we have a signifcantly-sized code base and we look 6 months to the past, we will see that changes tend to happen in the same places. We don\u0026rsquo;t change all types of features all the time and all types of infrastructure all the time.\nFeatures tend to evolve iteratively, and bugs that are introduced are statistically fixed in the following weeks or months.\nSo chances are - if you\u0026rsquo;ve changed an area of the code today, you will change it again in the next month.\nFor automated tests, what this means is that adding tests for the code we\u0026rsquo;re working on now tends to have a much larger impact than we might assume, because they are likely to protect us in the next few weeks.\nZero to one As with many, many considerations in any kind of project - making the first move is an uncomfortable \u0026ldquo;awkwardness\u0026rdquo; - we don\u0026rsquo;t exactly know how to do it.\nBut once we have a single test, we have an initial \u0026ldquo;paved road\u0026rdquo;, and we can keep improving in iterative steps, which is far easier.\nIf you never start, it\u0026rsquo;s 100% you\u0026rsquo;ll stay at zero Maybe the first test you\u0026rsquo;re writing will be the last.\nBut maybe it won\u0026rsquo;t, and it\u0026rsquo;ll lead to way to having a much better developer experience.\nIf you never write the first test, you will definitely not write tests number 2, 3 and 20.\nMy own experience I can\u0026rsquo;t back this up with anything besides \u0026ldquo;I\u0026rsquo;ve seen this enough, that\u0026rsquo;s the way it is\u0026rdquo; - but I can tell you that I almost never regretted writing the first few tests for a piece of code.\nIn almost all cases, it did help me improve the code design and prevent bugs.\nConclusion Start with something easy and simple, as long as you start :)\n","permalink":"https://shaigeva.com/posts/10_footguns/01_there_are_no_tests/","summary":"\u003cp\u003e(this post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eThis is a \u0026ldquo;warm-up footgun\u0026rdquo; to the blog post series.\u003c/p\u003e\n\u003cp\u003eThe easiest way to shoot yourself in the foot, testing-wise, is to have no tests at all.\u003c/p\u003e\n\u003cp\u003eIn my experience, writing any tests often helps us - even if these tests are not well-written, and even if they\u0026rsquo;re just a drop in the sea.\u003c/p\u003e\n\u003cp\u003eThere are a few reasons I noticed, why moving from no tests at all to even one test for some area of the code is useful.\u003c/p\u003e","title":"Footgun #1 - There Are No Tests"},{"content":"10 Ways To Shoot Yourself In The Foot With Tests - PyCon-IL 2024 (Hebrew) (English version below)\nThe talk shares hard-learned advice about how to avoid getting yourself in trouble with tests.\nSlides: Slideshare PyCon IL 2024\n10 Ways To Shoot Yourself In The Foot With Tests - PyCon-US 2023 (English) Given at Salt Lake City, Utah. The talk shares hard-learned advice about how to avoid getting yourself in trouble with tests.\nProperty-based testing - PyWeb-IL zoom meetup (Hebrew) General intro to property-based testing. Similar to the below talk but more python oriented, with more concrete examples. (and in zoom)\nProperty-based testing - Reversim2021 convention talk (Hebrew) General intro to property-based testing (not language specific)\n","permalink":"https://shaigeva.com/talks/","summary":"\u003ch1 id=\"10-ways-to-shoot-yourself-in-the-foot-with-tests---pycon-il-2024-hebrew\"\u003e10 Ways To Shoot Yourself In The Foot With Tests - PyCon-IL 2024 (Hebrew)\u003c/h1\u003e\n\u003cp\u003e(English version below)\u003c/p\u003e\n\u003cp\u003eThe talk shares hard-learned advice about how to avoid getting yourself in trouble with tests.\u003c/p\u003e\n\u003cp\u003eSlides: \u003ca href=\"https://bit.ly/testing_footguns_pycon_il_2024\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eSlideshare PyCon IL 2024\u003c/a\u003e\u003c/p\u003e\n\n\n    \n    \u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n      \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"allowfullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube.com/embed/k-vDmoPT84g?autoplay=0\u0026controls=1\u0026end=0\u0026loop=0\u0026mute=0\u0026start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\n      \u003e\u003c/iframe\u003e\n    \u003c/div\u003e\n\n\u003chr\u003e\n\u003ch1 id=\"10-ways-to-shoot-yourself-in-the-foot-with-tests---pycon-us-2023-english\"\u003e10 Ways To Shoot Yourself In The Foot With Tests - PyCon-US 2023 (English)\u003c/h1\u003e\n\u003cp\u003eGiven at Salt Lake City, Utah.\nThe talk shares hard-learned advice about how to avoid getting yourself in trouble with tests.\u003c/p\u003e","title":"Talks"}]