[{"content":"(this is the first post in a series about creating production-grade maintainable AI-first projects, using AI-first design patterns and frameworks)\nLike many others, I spent a lot of time thinking about AI and software development.\nI belong to the camp that believes that AI is a total paradigm shift - it\u0026rsquo;ll redefine the ecosystem and what it means to create software, and it\u0026rsquo;ll be the deepest change we have seen to date.\nMy own \u0026ldquo;flavor\u0026rdquo; of thinking about this is to try, from an engineering / implementation perspective, to understand what that change could look like.\nThis blog series will dig into this and share my thoughts and conclusions from experimentation.\nI\u0026rsquo;ll talk about both:\nWhat can we already do today? Concrete, pragmatic practices we can use right now to make ourselves faster. What would need to be different in how we work so in the future (few years?), everything will indeed be different and much more productive? My hope is that these will help enthusiastic devs boost their productivity, and to contribute to the general conversation around AI and coding.\nIn this intro post, I\u0026rsquo;m describing the general concept, with some high-level examples.\nCreating maintainable software By now (2025-06), the term vibe coding has been with us for a few months, and tools / platforms have been here for quite a bit more.\nAI coding assistants already help us in many ways, including creating impressive projects to a level that would seem science fiction just a few years ago.\nWe\u0026rsquo;re seeing fast and significant improvement in many areas of dev work.\nHowever, the effect is far less impressive for what is the bulk of dev work today - ongoing work on large code bases.\nDon\u0026rsquo;t get me wrong - it\u0026rsquo;s great, it\u0026rsquo;s an amazing productivity boost, but it\u0026rsquo;s not the same categorical change that we\u0026rsquo;re seeing when vibe coding smaller non-production-grade projects.\nWe all know that in most real-world non-small code bases, on most changes we make, AI just doesn\u0026rsquo;t speed us up 10x right now.\nIn practice, while this does happen sometimes - at the moment it\u0026rsquo;s only \u0026ldquo;here and there\u0026rdquo;. It\u0026rsquo;s not the general case.\nThe easy path is to adjust the code to the AI (AI-first / AI-native / AI-driven) I believe that huge improvements to productivity on large code bases are possible with the current generation of LLMs.\nMore than 10x.\nAnd I believe that part of what we need to do in order to get there in the long run is to build AI compatibility into the code-base from the ground up (this approach is usually called AI-first / AI-native / AI-driven development).\nFor the shorter-term, this can be \u0026ldquo;done in pieces\u0026rdquo; - make a certain part, or certain aspect of the code base be \u0026ldquo;AI-friendly\u0026rdquo;.\nWhy? Well, AI has different trade-offs than human teams in what is easy or hard when creating software.\nSo if we want AI to 10x what it can do (and what we can do), the easiest way is to build the code in a way that fits the AI\u0026rsquo;s tradeoffs.\nIn other words, we need to have definitions that dictate aspects of the code and workflow, that are AI-friendly. These can be very general and very simple (like \u0026ldquo;use meaningful variable names\u0026rdquo;), or very use-case specific, like how to use a specific framework in a specific language.\nSo anything might be relevant - workflow, specifications, tech stack, packages, tooling, coding conventions, code design, testing techniques, etc. etc.\nThe central point here is that we will not \u0026ldquo;continue to write code like we do now plus add AI\u0026rdquo;, but that we\u0026rsquo;ll see substantial changes to all of these.\nDesign patterns I\u0026rsquo;ll refer to these aspects of the code and workflow as \u0026ldquo;design patterns\u0026rdquo;.\nWhy \u0026ldquo;design patterns\u0026rdquo;? Well, there\u0026rsquo;s no real established terminology. Some of the things I\u0026rsquo;ll talk about are more \u0026ldquo;practices\u0026rdquo;, some are more \u0026ldquo;coding conventions\u0026rdquo; and some can only be called \u0026ldquo;design patterns\u0026rdquo;. So I\u0026rsquo;m going with the term that I think is best at conveying\nThe way we do code is different, it\u0026rsquo;s not just how we prompt or the tools we use\u0026quot;. It\u0026rsquo;s a design approach. It\u0026rsquo;s part of the design of our code, our workflow, our architecture, our team. A key part of being productive with AI, I believe, is to master these. How should we structure the code so AI can work with it well? What abstraction layers? What will the APIs between different components look like? Which tests should we have, at which layers?\nI\u0026rsquo;ll talk about examples (and do some POCs) of such design patterns in this series.\nIt\u0026rsquo;s worth mentioning that most (or all) concepts are not going to be too exotic - the approaches we\u0026rsquo;ll explore are variations of existing industry techniques.\nAn experienced developer will be familiar with many of them.\nThe point here is not to invent new ideas, but more to examine the option of applying what already exists in a systematic way to the general problem of programming with AI.\nFrameworks Individual design patterns are nice, and will be very beneficial for those that learn them.\nI believe, however, that the best way forward is what I think of as AI-first frameworks.\nVery loosely, what I mean by a framework is a \u0026ldquo;combo\u0026rdquo; of these design patterns plus relevant tooling.\nIt can be something like\nA Python backend service Enforces type annotations (Python generally allows, but doesn\u0026rsquo;t require, type annotations. So a framework could require it). Using a spceific backend library (e.g. Python\u0026rsquo;s FastAPI) Has pre-defined layers of abstraction. E.g. \u0026ldquo;there is a DAL (data access layer), and all its APIs only receive and return immutable data structures\u0026rdquo;. There are pre-defined layers where tests are written, and they have specific technical requirements. E.g. \u0026ldquo;the DAL must have a robust test suite for testing every single workflow. That test suite only uses the DAL API (can\u0026rsquo;t use SQL or an ORM directly)\u0026rdquo;. etc. etc. (there will probably be dozens of these, incl. specific commands to how we run some tools, AI agent rule files and whatever\u0026rsquo;s needed to enforce the specification) These frameworks will create a \u0026ldquo;cohesive whole\u0026rdquo; that an AI agent can work with effectively.\nFor example, definitions in the spirit of what I gave above (but, of course, much more robust) would force the AI agent to create a well-defined set of all possible interactions with the database and have tests that thoroughly cover them.\nBut why do we need these \u0026ldquo;frameworks\u0026rdquo;?\nWell, having each team hand-craft their own AI-compatible practices for their code base is kind of like having each backend team develop their own web framework from low-level http libraries, just because \u0026ldquo;we want it to be tailored to our use case\u0026rdquo;.\nIt can be done, but we all understand it\u0026rsquo;s not a good idea. It\u0026rsquo;s very expensive and the result will suck most of the time.\nI think it\u0026rsquo;ll be much better if the industry will \u0026ldquo;think of it\u0026rdquo; a little bit like we think of web development today: there will be common (open source?) frameworks / tool-sets created by experts, and most teams will just use a combination of a few standard options.\nTL;DR There are a lot of pieces to this, and it\u0026rsquo;ll take a while until I manage to release blog posts covering everything I have in mind, so it\u0026rsquo;s worth it to give a very high level view of the approach here:\nPrinciples The most significant mental models here are the feedback loop and the bug funnel.\nThe bug funnel (it\u0026rsquo;s not really just bugs - also features etc., but \u0026ldquo;bug funnel\u0026rdquo; is easy to think about) is the concept that when code is written (by a human or AI), it sometimes contains things that are not desirable.\nAs that code \u0026ldquo;moves forward in the software lifecycle\u0026rdquo; (compilers, linters, various tests, review, etc. etc.), some of these bugs are discovered at every step and get filtered out.\nBugs that are discovered earlier in the \u0026ldquo;funnel\u0026rdquo; are cheaper than bugs found later.\nIn other words, it\u0026rsquo;s a very good idea to \u0026ldquo;shift-left\u0026rdquo; bugs in the bug funnel.\nThe feedback loop is what we all know - planning, doing, verifying and then doing it again and again until we\u0026rsquo;re done.\nA fast and effective feedback loop has always been one of the most significant things in the process of creating software, and AI assistants will magnify that by orders of magnitude.\nOur main objective is to make the human feedback loop \u0026ldquo;better\u0026rdquo; (faster, easier, more effective), and the main way that we\u0026rsquo;ll try to achieve this is by working very hard so that the AI agents will have their own internal feedback loop.\nMeaning - the agent will plan, make changes and then verify them itself and auto-heal them, and only after it\u0026rsquo;s done - it\u0026rsquo;ll give the result to a human.\nIn other words, the \u0026ldquo;star of the show\u0026rdquo; is going to be the ability to create fast feedback loops that are effective at both creating new things and shifting-left bugs in the bug funnel.\nMy main focus will be on the verification part - allowing the AI to check the changes it makes.\nWe\u0026rsquo;ll explore a bunch of directions like linting, static typing approaches, ideas like automatic screenshots and LLM automatic reviews. And mostly - A LOT of code design and testing techniques.\nPractices / techniques As mentioned, our main technical objective will be a fast AI-internal feedback loop that \u0026ldquo;rule-out\u0026rdquo; as many \u0026ldquo;bugs\u0026rdquo; as possible, as quickly as possible.\n\u0026ldquo;Quick\u0026rdquo; means that almost all \u0026ldquo;bugs\u0026rdquo; can be ruled-out in a few seconds by the AI without human intervention.\nThere will, of course, also be slower verifications like e2e tests - but most bugs should be caught by the faster tests, earlier in the process.\nThis has very strong implications on what practices are expected to be effective.\nFor instance:\nWe need a setup that allows the AI agent to run code to check the changes it makes (sometimes the entire program, sometiems only tests). Having an LLM just \u0026ldquo;review\u0026rdquo; written code just ain\u0026rsquo;t gonna cut it. But - we should try hard to verify things about the code even without running it (as a test or otherwise). Examples: Static typing, of course. This can be taken further than most people are aware and AI is a good match for this. Preference for pure functions where applicable. Have simulators for most side-effects, especially those that we don\u0026rsquo;t directly control. A simulator is a simplified implementation of some part of a real thing, that has a very similar behavior. For example, a DB table can be a list of in-memory tuples plus some wrappers. AKA \u0026ldquo;fakes\u0026rdquo; in standard test-speak. Required because side-effects might be unsafe, unreliable, uncontrollable and slow, but we must allow the AI to run code (either as a test or not) in a way that is safe-enough, reliable-enough, controllable-enough and fast-enough. This is an almost logical necessity, even though it\u0026rsquo;s not \u0026ldquo;part of the conversation\u0026rdquo; now at all (I\u0026rsquo;m not sure I\u0026rsquo;ve ever seen it mentioned in the AI-building conversation, actually). Without this, everything\u0026rsquo;s going to be much harder. I will be talking a lot about this, since bringing this down to reality is a challenge. Very strong preference towards small building blocks that compose into larger components where possible (stronger than would be appropriate for many human teams). This helps to have \u0026ldquo;divide and conquer\u0026rdquo; of bugs, leaving as few bugs as possible to the more complex, slower tests. Lastly, of course - we need to actually use all of this to design and create coding+testing strategies that have a good ROI. This point is the most vague, the most nuanced, but also probably the most important because it\u0026rsquo;s necessary, and I think it\u0026rsquo;s pretty difficult. I\u0026rsquo;ve had these ideas running around in my head for while and at least for me they are interesting, so I felt like it\u0026rsquo;s time to share.\nI hope you find this useful, or at least interesting.\nPing me on social (twitter / x, linkedin) and let me know!\nIn the next post (coming in a week, I hope), I\u0026rsquo;ll give an example of what an internal AI feedback loop looks like on a small project, and start looking at some of the more basic techniques.\n","permalink":"https://shaigeva.com/posts/ai_frameworks/01_ai_frameworks_intro/","summary":"\u003cp\u003e(this is the first post in a series about creating production-grade maintainable AI-first projects, using\nAI-first design patterns and frameworks)\u003c/p\u003e\n\u003cp\u003eLike many others, I spent a lot of time thinking about AI and software development.\u003c/p\u003e\n\u003cp\u003eI belong to the camp that believes that AI is a total paradigm shift - it\u0026rsquo;ll redefine the ecosystem and what it means to\ncreate software, and it\u0026rsquo;ll be the deepest change we have seen to date.\u003c/p\u003e","title":"Moving past vibe-coding? In Search of real-world AI-First Development: AI-First Design Patterns and Frameworks (blog post series)"},{"content":"This is a series of posts, following a talk I gave (twice - at Pycon-US 2023 and Pycon-IL 2024), about testing best (and not-so-best) practices.\nThe talk shares 10 practices that I had bad experience with, along with ways of avoiding them.\nStarting with simple (but useful!), and moving on to more complex ideas:\nThere are no tests (warm up) Untested tests The tests are not isolated No locality of behavior Unclear language Testing too many things Improper test scope Test doubles everywhere Slow tests Wrong priorities Videos and slide decks from the talks\n10 Ways To Shoot Yourself In The Foot With Tests - PyCon-US 2023 (English) Given at Salt Lake City, Utah.\nSlides: Slideshare PyCon US 2023\n10 Ways To Shoot Yourself In The Foot With Tests - PyCon-IL 2024 (Hebrew) Slides: Slideshare PyCon IL 2024\n","permalink":"https://shaigeva.com/posts/10_footguns/ten_footguns/","summary":"\u003cp\u003eThis is a series of posts, following a talk I gave (twice - at Pycon-US 2023 and Pycon-IL 2024), about testing best (and not-so-best) practices.\u003c/p\u003e\n\u003cp\u003eThe talk shares 10 practices that I had bad experience with, along with ways of avoiding them.\u003c/p\u003e\n\u003cp\u003eStarting with simple (but useful!), and moving on to more complex ideas:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/01_there_are_no_tests/\"\u003eThere are no tests\u003c/a\u003e (warm up)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/02_untested_tests/\"\u003eUntested tests\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/03_the_tests_are_not_isolated/\"\u003eThe tests are not isolated\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/04_no_locality_of_behavior/\"\u003eNo locality of behavior\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/05_unclear_language/\"\u003eUnclear language\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/10_footguns/06_testing_too_many_things/\"\u003eTesting too many things\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eImproper test scope\u003c/li\u003e\n\u003cli\u003eTest doubles everywhere\u003c/li\u003e\n\u003cli\u003eSlow tests\u003c/li\u003e\n\u003cli\u003eWrong priorities\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eVideos and slide decks from the talks\u003c/p\u003e","title":"10 Ways To Shoot Yourself In The Foot With Tests"},{"content":" (this mini-post is part of a series about good testing practices)\nJust like with product code, if we put too many things in the same place we get a mess.\nMy rule of thumb is to try hard to test a single fact about the behavior of the code. And it helps if I use these specific words mentally.\nSINGLE. FACT. About the BEHAVIOR.\nExample Let\u0026rsquo;s continue with our example from the previous post, and say we have a book store and we\u0026rsquo;re testing the edit book functionality.\nFor example, that\u0026rsquo;s a single fact about the behavior the code: test_user_can_edit_their_own_book And, this is not a single fact, it\u0026rsquo;s too general: test_edit_book How do they compare? Easy to understand? Single fact test: It\u0026rsquo;s clear what the test checks. It\u0026rsquo;s clear that it only checks that. General test: we\u0026rsquo;ll need to read and understand all the test code to know. Easy to debug? Single fact test: If it fails, it\u0026rsquo;s clear what functionality stopped working. And because it\u0026rsquo;s small, it\u0026rsquo;ll be easy to debug it. General test: if it fails, anything related to edit book might have failed. We\u0026rsquo;ll need to dig in. And it does a lot of things, so debugging might be a lot of work. Conclusion Try to have each test case will test a SINGLE FACT about the BEHAVIOR of the code.\nThis makes a huge difference and it\u0026rsquo;s worth it to invest a lot into this.\n\u0026lt;\u0026lt; previous post: Unclear Language | (coming soon) next post: Improper Test Scope \u0026gt;\u0026gt; ","permalink":"https://shaigeva.com/posts/10_footguns/06_testing_too_many_things/","summary":"\u003cstyle\u003e\n.code-example {\n  background-color: #2E2E33;\n  padding: 10px;\n  margin-bottom:10px;\n  border-radius: 5px;\n  font-family: monospace;\n  white-space: pre;\n  color: #d5d5d6;\n  font-size: .78em;\n  line-height: 1.5;\n}\n.highlight-red {\n  color: red;\n}\n.highlight-green {\n  color: green;\n}\n\u003c/style\u003e\n\n\u003cp\u003e(this mini-post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eJust like with product code, if we put too many things in the same place we get a mess.\u003c/p\u003e\n\u003cp\u003eMy rule of thumb is to try hard to test a single fact about the behavior of the code.\nAnd it helps if I use these specific words mentally.\u003c/p\u003e","title":"Footgun #6 - Testing Too Many Things"},{"content":"(this is the first post in a series about creating production-grade maintainable AI-first projects, using AI-first frameworks)\nIn this post I\u0026rsquo;ll focus on the reason I think frameworks are the direction and what I think they will include, where following posts will dive deeper into the technical details.\nThe objective here is not to \u0026ldquo;convince\u0026rdquo; (that would not fit in a blog post :) ), but to lay out the main points.\nAt a very high level, the rationale is this:\nIt makes sense to \u0026ldquo;aim our coding for the AI\u0026rdquo;, because it\u0026rsquo;ll be the new norm. With the current way we\u0026rsquo;re making software, it\u0026rsquo;s just not realistic to make a paradigm shift happen, because AI cannot have a feedback loop that\u0026rsquo;s good enough to change code at an acceptable speed and self-heal enough issues. Therefore, I believe the best forward is to have frameworks that dictate some important aspects of the code and workflow, in order to give AI what it needs so it can do its job. Let\u0026rsquo;s dig in.\nAn industrial revolution Soon enough, the vast majority of code will be written by AI.\nSo in a sense, this is a large automation movement.\nThings human craftsmen currently make will be handed over en masse to the machine.\nGiven that, it makes sense to think about the common case for software creation, which will soon be the automated process.\nAnd as with other automations, the automation might involve a redesign.\nThe printing press is not a faster pen. An assembly line is not a faster human craftsman. They create in a different way, and their outcome is a little different.\nAnd we should expect the same from automating software creation.\nA reality of imperfection There\u0026rsquo;s a fundamental truth about \u0026ldquo;making things\u0026rdquo;: it\u0026rsquo;s not going to be perfect.\nThe specification might not be clear, some implicit assumption might be incorrect, and of course a simple mistake might sneak in. Randomness will happen.\nWe are imperfect humans in imperfect teams, building imperfect products in an imperfect reality.\nAnd it is the same with AI. LLMs have limits, and they have randomness built in.\nSo when AI makes a change, sometimes it won\u0026rsquo;t be what we need.\nIt\u0026rsquo;s not going to be the same imperfections a human creates, but it\u0026rsquo;ll happen.\nThe almighty feedback loop The way to deal with this reality is to iterate. We all know this, it\u0026rsquo;s our day-to-day.\nWe plan - decide what to do next. Then we do the thing. We verify - get feedback (check if that thing is a step in the right direction). Then, given that new information from the feedback - we go back to the beginning.\nPlan Do Verify And again and again\u0026hellip;\nFor me, this feedback loop is the main mental model for thinking about software development processes.\nProduct might iterate on UX details, a complex project would be built in iterations.\nAnd so would an individual development task.\nYou write some code, then you test and fix and test again. Deploy to staging and test some more, fixing again if you find a new bug etc. etc.\nThis is an essential part of dealing with imperfection or randomness.\nIf things sometimes go wrong, we need to know to spot the error and fix it.\nIt\u0026rsquo;s always an auto-correcting feedback loop.\nThe size of an iteration The shorter a feedback loop is, the better.\nAny mistake can send you off in the wrong direction, so you want to get feedback as early as possible.\nA simple example that can help visualize why this is important would be to imagine writing code for an entire week without ever running it, compiling it or getting warnings from the IDE.\nWe all know that the result is going to be pretty nasty. No serious developer will choose to work like this.\nThis matters A LOT (personally, \u0026ldquo;iteration size\u0026rdquo; is the main \u0026ldquo;metric\u0026rdquo; I look at for improving productivity).\nAI and our feedback loops Although not always discussed this way, a lot of what we\u0026rsquo;re doing now with AI tools is integrating them into our own feedback loop.\nAnd when you look at it from this angle, you can see that a lot of the techniques we use to improve the performance of human+ai are just optimizing some part of our feedback loop - adding steps in the process for detailed planning, breaking down to smaller tasks, making sure the correct context gets in so it\u0026rsquo;s easier for the AI to \u0026ldquo;do\u0026rdquo; etc.\nAI\u0026rsquo;s internal feedback loop today An observation I\u0026rsquo;m making here is that the AI has (or can have) its own feedback loop before it hands the latest change over to us.\nMy own experience, and I\u0026rsquo;m sure many others\u0026rsquo; as well, is that the best outcome I get is if I set up an AI assistant for a feedback loop. Each step looks like this:\nUnderstand what to do next Create something small Verify. For example, write a new test + Run all relevant tests. If something breaks - the next action would be to fix it. I let this run automatically in a loop until either all tests are green, it gives up or it starts going crazy.\nThis is possible, for example with Cursor, for some time now. In fact, Cursor will try to self-heal what it can out of the box - stuff like lint errors.\nThis works extremely well. The AI having its own internal self-healing feedback loop, backed by the right tests to make me feel safe-enough that nothing breaks, is a different category of productivity.\nSome bugs still get through to me. I test, but it\u0026rsquo;s kind of \u0026ldquo;skipping to the end of the process\u0026rdquo; - I would also do the same tests if I wrote the code, and find similar bugs.\nWhen I can set it up, it\u0026rsquo;s just great. But\u0026hellip; for most of my work, I can\u0026rsquo;t.\nAI\u0026rsquo;s internal feedback loop tomorrow When I distill the situation as far as I can, this is what remains.\nAn LLM will statistically do the wrong thing sometimes. Large, standard codebases and systems are FAR too complex for these \u0026ldquo;imperfections\u0026rdquo; to be rare. So if we want to avoid most of these bugs reaching a human - we must allow the AI to self-correct using a feedback loop. A feedback loop is therefore sort-of a \u0026ldquo;constant\u0026rdquo; in this \u0026ldquo;movement to AI\u0026rdquo;. A requirement that must be satistified for large, ongoing projects.\nIt follows that at least for complex projects (though I would say for almost all projects), our best bet is to adopt coding practices that work well with such a feedback loop.\nThese kinds of coding practices are what I call AI-first frameworks.\nWhat are some limiting factors? So, why don\u0026rsquo;t we have such feedback loops for our projects today?\nWhy don\u0026rsquo;t we just tell the AI \u0026ldquo;here\u0026rsquo;s a project with a million lines of code. Add a small feature\u0026rdquo;, and then it would do changes and verify them itself?\nIf you\u0026rsquo;re following the trend on social media, you can see that improvements happen all the time, both in tools and how to use them.\nBut if we look from the perspective of the plan-do-verify stages, we can see that almost everything falls between \u0026ldquo;plan\u0026rdquo; and \u0026ldquo;do\u0026rdquo;.\nThere are definitely improvements in \u0026ldquo;verify\u0026rdquo; (and companies that emphasize quality*), but for the \u0026ldquo;general project\u0026rdquo; there really is no big news. AI assistants will generate code and if you ask they\u0026rsquo;ll generate some tests. But there\u0026rsquo;s no tool that you can throw on your project that would make changes and make sure nothing broke.\nSo\nTwitter is full of tips and tricks on rule files and project documentation, workflows that break down work into small steps etc.\nThe way I see it, we have two significant bottlenecks:\nFirst: understanding complex code well, incl. flow execution. I believe there are codebases where this works pretty well, but certainly not consistently enough.\nAnd second: the real limiting factor of the AI software movement - \u0026ldquo;verify\u0026rdquo;.\nThe tests.\nWell, mostly the tests - there are other verifications.\nFor example, automatically taking a one-time screenshot of a UI component \u0026ldquo;before and after\u0026rdquo; and comparing is very much a verification, though it\u0026rsquo;s not considered a \u0026ldquo;test\u0026rdquo; by common terminology. But I\u0026rsquo;ll usually include all verification in \u0026ldquo;tests\u0026rdquo; to be a little less verbose.\nThe tests I\u0026rsquo;m proficient with tests. I have given testing a lot of attention for two decades, I give talks about tests, I had successes and failures and I have an \u0026ldquo;arsenal\u0026rdquo; of effective techniques.\nBecause of that, I know it\u0026rsquo;s not realistic to add good tests to most code bases. At least, good tests that will be fast enough.\nCode is usually not testable if it\u0026rsquo;s not designed to be testable.\nSome common issues are\nLogical complexity, where some component does a lot of stuff and it\u0026rsquo;s difficult to isolate what you want to test in a way that\u0026rsquo;s both reliable and simple enough. Especially for complex flows that include multiple steps and many components with non-trivial state changes. Technical complexity, where it\u0026rsquo;s difficult to set up things. A simple example would be data samples for complex processes, but the more hairy issues involve things like like relying on some external thing for which it\u0026rsquo;s tricky to simulate failure modes. Side-effects, where the code interacts with the outside world in some way. Consider something like this: we have an arbitrary microservice architecture with 50 repos, where some services are written in typescript, some are written in python without type hints, and the team has even braved Rust on one of them. We mostly use Postgres with redis, but some workflows use s3. 5 3rd party APIs handle a few concerns like payments, opening support tickets or similar. We might not even have a trivial way to set up \u0026ldquo;new clean test\u0026rdquo; on a local machine. Now, the AI made a significant change to a shared python library that deals with the DB.\nWhen you call the typescript web service, which will then call 15 different services, how easy will it be for the us to trust that all read-only transactions stay read-only and that the latest change doesn\u0026rsquo;t accidentally override a value in some obscure sequence of events where Stripe returns 502 once every 3 weeks? We can\u0026rsquo;t run all the options. It might not even be possible to technically test all failure modes for all 3rd party services that we use. And would it not run for 30 years if we tried?\nAnd since we\u0026rsquo;re not testing it out, what will we do? Do you trust any model that tells you \u0026ldquo;I\u0026rsquo;ve seen all the context, there\u0026rsquo;s no sequence of events that triggers a dormant bug from 2 years ago and breaks this\u0026rdquo;?\nWe need to trust it, or this doesn\u0026rsquo;t work.\nNow, there are solutions, but they require specific design patterns and tooling.\nI\u0026rsquo;ll finish this point here, because this is already a very long post.\nIf what I wrote here has not convinced you - let\u0026rsquo;s mark this as a debt to be explained, and one of the planned posts in the series will take a deep-dive into this point.\nFor now, let\u0026rsquo;s assume that adding good, fast tests to an arbitrary code base is very hard.\nHard enough that to make the paradigm shift it\u0026rsquo;s easier to switch coding styles than it is to create tools that can deal with the old code style.\nNOTE: maybe say something like If you want to make sure that an endpoint is read only, you can do ad-hoc static analysis to try to make sure that there is no code path where an sql query that changes data runs, you can write thousands of tests covering many possible scenarios for what might have been in the DB to begin with etc.\nYou can also use some permissions framework and only give that endpoint read-only access to the DB. Then, in order to test that \u0026ldquo;endpoint_x doesn\u0026rsquo;t write to the DB\u0026rdquo; we just need to make sure that we use the permission framework and that the endpoint was indeed assigned read-only permissions.\nIt\u0026rsquo;s not a very common approach today, but it\u0026rsquo;s easy to see how it eliminates categories of bugs very easily and very reliably.\nSecurity I\u0026rsquo;d be remiss if I didn\u0026rsquo;t mention security.\nEven for something as simple as testing locally - the disk on our machine typically has a .env file with secrets.\nAutomatically running a unit test that was created automatically, on code that was created automatically might leak that.\nSo if we want to be able to just let the AI do its thing with minimal supervision, it might be a good idea to have some guardrails.\nFrameworks It therefore seems that the most make-sense way forward, is to build AI-compatibility into every part of software creation - design, coding, workflows, security etc.\nIf tests on a general codebase are hard, let\u0026rsquo;s have a design that guarantees our codebases can be tested.\nIf we need to avoid leaking information, we probably need to address that.\nIt\u0026rsquo;s difficult enough to solve these issues if we \u0026ldquo;own\u0026rdquo; the tech stack and design, and can freely run any part of the code that we want.\nIf we don\u0026rsquo;t, it just becomes exponentially more difficult.\nWe need to understand what AI needs, and give it that, instead of trying to have it \u0026ldquo;just work\u0026rdquo; on whatever arbitrary code base we happen to have.\nThis is what I think of as an AI-native framework.\nIt would take a certain use case (hopefully relatively broad) and design some of the of the aspects of the project to make them AI-compatible.\nI don\u0026rsquo;t know what would be the granularity of these frameworks\nMaybe it\u0026rsquo;ll be a single idea like testing at a certain layer in an http web server and you would compose a bunch of these together Maybe it\u0026rsquo;ll be \u0026ldquo;FastAPI + Postgres + React has at least these layers and tests that look like these\u0026rdquo; Maybe it\u0026rsquo;ll be much more generic than that. Maybe the only frameworks we see will be part of paid platforms that also contain the AI engines that use the frameworks. You could satisfy a lot of these requirements by having something like Wix and have custom-generated-code hook into different parts of the system. I don\u0026rsquo;t think this is where it\u0026rsquo;s going, but maybe. What I am sure of is that frameworks will set up software projects so that AI engines can have effective feedback loops.\nThey will include at least some strict design and testing guidelines, and will probably include ways to document and declare a spec for parts of the software.\nI do believe the successful ones will probably be technology-specific, at least to some degree.\nThere are things we can do with some technologies that we can\u0026rsquo;t do with others, and they matter.\nFor example, we don\u0026rsquo;t have static types in javascript (put aside jsdoc for a sec), but we do have them in typescript. This changes some tradeoffs of what\u0026rsquo;s easy for the AI to self-heal and what is not.\nI believe some of the conventions we see in these frameworks will be very similar to what some teams already do.\nBut some will not. It is easier for a machine to do some things that are difficult for us and vice versa.\nFor example, the way it makes sense for an AI to use types is typescript is not the same in my opinion as the common conventios. I\u0026rsquo;ll dig into this quite a bit later on.\nSome tooling will almost certainly be very different from today.\nI don\u0026rsquo;t think we\u0026rsquo;ll have Python code generation at scale without at least something like a sandbox be part of common frameworks. Maybe even the ability to easily configure file access, limitations on network calling etc.\nWrapping up I hope I managed to make the case that it\u0026rsquo;ll very difficult to have industrial-strength code generation at scale for arbitrary code bases.\nAnd that we must therefore explore frameworks that would allow us to control enough of the structure of the code that would allow the AI to have an effective internal feedback loop.\nIn the next post, we\u0026rsquo;ll start exploring an example of what a simple framework might look like.\nAbout companies that emphasize quality - I\u0026rsquo;m biased, as I worked there in the past, but if you\u0026rsquo;re interested you might want to have a look at Qodo. \u0026lt;\u0026lt; previous post: AI-First Development Frameworks (intro) | next post: (coming soon) \u0026gt;\u0026gt; ","permalink":"https://shaigeva.com/posts/ai_frameworks/03_why_ai_frameworks/","summary":"\u003cp\u003e(this is the first post in a \u003ca href=\"https://shaigeva.com/posts/ai_frameworks/01_ai_frameworks_intro/\"\u003eseries\u003c/a\u003e about creating\nproduction-grade maintainable AI-first projects, using AI-first frameworks)\u003c/p\u003e\n\u003cp\u003eIn this post I\u0026rsquo;ll focus on the reason I think frameworks are the direction and what I think they will include,\nwhere following posts will dive deeper into the technical details.\u003cbr\u003e\nThe objective here is not to \u0026ldquo;convince\u0026rdquo; (that would not fit in a blog post :) ), but to lay out the main points.\u003c/p\u003e","title":"Why AI Frameworks?"},{"content":" (this mini-post is part of a series about good testing practices)\nAnother problem that makes it more difficult to understand tests is unclear language.\nTwo guidelines that help me deal with this:\nWe want to use decisive language We want the language to be specific and explicit Suppose we have a book store and we\u0026rsquo;re testing the functionality for editing a book.\nLet\u0026rsquo;s see some examples of test phrasing:\ndef test_edit_book(): ... This is simply too general.\nThere are so many things that might be tested, and this means almost nothing about what will get tested in practice.\ndef test_edit_book_works_correctly(): ... Adding things like \u0026ldquo;it works\u0026rdquo; or \u0026ldquo;it\u0026rsquo;s correct\u0026rdquo; - most of the time, this is just bloat.\nIt only makes the name bigger, but doesn\u0026rsquo;t give us any extra information.\ndef test_user_should_be_able_to_edit_their_own_book(): ... That\u0026rsquo;s much better - it\u0026rsquo;s a lot more specific.\nThe only problem here is the indecisive language.\nWhy \u0026ldquo;should\u0026rdquo;?\nWill this ever NOT be correct?\nIt\u0026rsquo;s both bloated and confusing.\nSo this as well - not optimal.\ndef test_user_can_edit_their_own_book(): ... That\u0026rsquo;s much better in my opinion.\nIt\u0026rsquo;s decisive, explicit and specific.\nI suggest to aim towards this whenever possible.\nConclusion When phrasing test names and descriptions, try to aim for decisive, specific and explicit language.\n\u0026lt;\u0026lt; previous post: No Locality of Behavior | next post: Testing Too Many Things \u0026gt;\u0026gt; ","permalink":"https://shaigeva.com/posts/10_footguns/05_unclear_language/","summary":"\u003cstyle\u003e\n.code-example {\n  background-color: #2E2E33;\n  padding: 10px;\n  margin-bottom:10px;\n  border-radius: 5px;\n  font-family: monospace;\n  white-space: pre;\n  color: #d5d5d6;\n  font-size: .78em;\n  line-height: 1.5;\n}\n.highlight-red {\n  color: red;\n}\n.highlight-green {\n  color: green;\n}\n\u003c/style\u003e\n\n\u003cp\u003e(this mini-post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eAnother problem that makes it more difficult to understand tests is unclear language.\u003c/p\u003e\n\u003cp\u003eTwo guidelines that help me deal with this:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWe want to use decisive language\u003c/li\u003e\n\u003cli\u003eWe want the language to be specific and explicit\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSuppose we have a book store and we\u0026rsquo;re testing the functionality for editing a book.\u003cbr\u003e\nLet\u0026rsquo;s see some examples of test phrasing:\u003c/p\u003e","title":"Footgun #5 - Unclear Language"},{"content":"(this mini-post is part of a series about good testing practices)\nOne testing problem that doesn\u0026rsquo;t get enough attention in my opinion is tests that don\u0026rsquo;t have locality of behavior.\nBy that I mean cases where a test is broken down into different parts in a way that makes understanding more difficult.\nThis is important in every type of code, and tests are no exception.\nExample: non-local data Consider this test:\ndef test_something(): data = Path(PATH_TO_DATA_FILE).read_text() assert calc_something(data) == 4.5 The data that the test uses is in a different file, so in order to understand the test we will need to locate that file and open it.\nEven if the data was in the same file, but a different place - it would still be an issue.\nNow, sometimes we don\u0026rsquo;t have a choice, and it\u0026rsquo;s the only way to do it.\nBut sometimes we do.\nFor example, if we can find a data example that\u0026rsquo;s small enough, we can do something like this:\ndef test_something(): data = “”” { \u0026lt;JSON data\u0026gt; } “”” assert calc_something(data) == 4.5 This is exactly the same test, but the data is local so it\u0026rsquo;s going to be much easier to understand at a glance, without \u0026ldquo;breaking the flow\u0026rdquo;.\nIt\u0026rsquo;s easier in tests than in production code One of the main problems with achieving locality of behavior is that it conflicts with DRY (\u0026ldquo;don\u0026rsquo;t repeat yourself\u0026rdquo;).\nWhat\u0026rsquo;s the problem with code duplication in production code?\nYou\u0026rsquo;ll often hear people talking about the \u0026ldquo;economics\u0026rdquo; - if you repeat a piece of code 3 times, then if you need to change that logic, you would need to do that work 3 times.\nHowever, this is actually a secondary consideration, especially if the number of repetitions is not high (let\u0026rsquo;s say 5 or less).\nThe real issue with repeating yourself is that duplication is an implicit dependency.\nIf you repeat the same logic in 3 different places, there\u0026rsquo;s a risk that if the logic needs to change, you would not notice one of these places, which would cause that \u0026ldquo;usage\u0026rdquo; to be deprecated and incosistent with the rest of the code - which will result in bugs and maintainability overhead, of course.\nThe nice thing about tests here is that this consideration is weaker, because of several factors.\nWhen we change code that has a test and make it behave differently, the relevant tests will usually break, so we will have something that points out to the duplicated \u0026ldquo;usages\u0026rdquo;. Where in production code - we only get that benefit if that piece of code has relevant tests - which might be very far from \u0026ldquo;always\u0026rdquo;. If the tests are focused and verify only a single fact, the number of times that we have duplication will be lower. And it\u0026rsquo;s far easier to write a test (at least a test that\u0026rsquo;s not end-to-end) that \u0026ldquo;checks one thing\u0026rdquo; than it is to write code that \u0026ldquo;does a single thing\u0026rdquo;, because if a piece of code does multiple things - you can just write multiple tests that run it separately, and each of them would test one thing. And, finally, if the mistake does happen in test-code and we forget to update a \u0026ldquo;usage\u0026rdquo; - it\u0026rsquo;ll render the test \u0026ldquo;wrong\u0026rdquo;, but it won\u0026rsquo;t actually cause a bug. Making a test wrong is bad, but it\u0026rsquo;s not as bad as a production bug, unless it affects a lot of tests. Of course, duplication is still something to consider - if there\u0026rsquo;s complex setup, or something that\u0026rsquo;s not complex but repeats many times - it\u0026rsquo;s probably worth it to extract functionality.\nThe point is that in production code, the right time to extract functionality is often after two or three usages - but in tests the balance allows for more.\nConclusion Be aware that tests which are \u0026ldquo;non-local\u0026rdquo; are far more difficult to maintain, and make a conscious effort to find ways to reduce the problem.\n\u0026lt;\u0026lt; previous post: The Tests Are Not Isolated | next post: Unclear Language \u0026gt;\u0026gt; ","permalink":"https://shaigeva.com/posts/10_footguns/04_no_locality_of_behavior/","summary":"\u003cp\u003e(this mini-post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eOne testing problem that doesn\u0026rsquo;t get enough attention in my opinion is tests that don\u0026rsquo;t have locality of behavior.\u003c/p\u003e\n\u003cp\u003eBy that I mean cases where a test is broken down into different parts in a way that makes understanding more difficult.\u003c/p\u003e\n\u003cp\u003eThis is important in every type of code, and tests are no exception.\u003c/p\u003e\n\u003ch2 id=\"example-non-local-data\"\u003eExample: non-local data\u003c/h2\u003e\n\u003cp\u003eConsider this test:\u003c/p\u003e","title":"Footgun #4 - No Locality of Behavior"},{"content":"(this post is part of a series about good testing practices)\nWriting tests that are not isolated is a sure way to create unnecessary work for ourselves.\nBy \u0026ldquo;tests that are not isolated\u0026rdquo;, I mean tests that sometimes have a different outcome (failing / passing) if we run only a subset of them, if we run them in a different order or if we run them in parallel.\nWhy is this a problem? Let\u0026rsquo;s say we have 30 tests, and test 24 passes if we run it individually but fails if we run the entire test suite.\nIf the test was isolated, then there are only a limited number of things that can go wrong. The things that the test actually executes will almost certainly be the cause of the failure.\nBut now, of course, the reason for failure is not what test 24 checks, and not what any of the other tests checks either. The failure is caused by an implicit interaction between test 24 and, say, test 8.\nThis is difficult to debug, of course, because we would first need to play detective and find that it\u0026rsquo;s test 8 that\u0026rsquo;s causing the problem (which might take a LOT of effort, especially if the tests are e2e tests that run slowly or only in CI), and then track the combined flow of the tests to find the problematic interaction.\nBut it\u0026rsquo;s worse than this - what often happens is that we run the entire test suite, either locally or in CI, and test 24 fails - so we start analyzing it as if test 24 fails individually. We don\u0026rsquo;t even consider that it\u0026rsquo;s a combined failure, and sometimes we waste hours chasing irrelevant clues, just to get to the point that we notice the test passes if its executed separately.\nSo the fact that we have \u0026ldquo;individual tests\u0026rdquo; is actually a misleading illusion that only distracts us from the unfortunate truth - we have one gigantic test with inter-connected sub-sections.\nIf e2e tests run in parallel and sometimes run in different order, which makes this cross-test-failure flaky, we get the perfect storm and we might spend days of work on this.\nAnd, lastly, the situation could actually be worse - it might be that test 24 SHOULD fail, but it passes because of test 8, hiding a bug.\nWhat are the causes behind this? There are rare cases like technical limitations (e.g. external service rate limits), but in the vast majority of cases - it\u0026rsquo;s shared mutable state.\nA typical scenario for unit tests would be a global in-memory object, and for end-to-end tests maybe a row in a database.\nTest 8 would change that shared thing, and test 24 would therefore have a different state when it starts to execute, leading to the failure.\nSharing a resource that\u0026rsquo;s immutable (or never changes in practice) is not a problem - if it never changes, then it\u0026rsquo;s the same whether or not it\u0026rsquo;s accessed by multiple \u0026ldquo;consumers\u0026rdquo;.\nI\u0026rsquo;ve also seen more subtle cases of shared mutable state, for example tests that would rely on an email being sent through an actual email-sending service, and multiple tests checked for the same email.\nWhat can we do? First of all, because in my experience this is so often a very painful problem - I suggest to avoid this from the start, even if it means extra work or delaying writing some tests if can\u0026rsquo;t make them isolated at the moment.\nDO NOT create a suite of non-isolated e2e tests that use long-living mutating database entities. This is very very likely to cause a crisis down the road.\nBecause the problem is usually shared mutable state, we mostly just need to avoid that specific issue.\nUnit tests can use the same in-memory objects, but they have to be immutable, or at least never changed by convention.\nIf this is not an option - either create the object in every test or have some original which you clone at the beginning of the tests.\nThe trickier part are higher-level tests that use expensive resources like a database, where full initializations might cause the tests to be too slow.\nIf it\u0026rsquo;s not too expensive - initialize the entire thing, of course.\nOtherwise - make sure to clean up - delete created resources and revert the database / file system / etc. to its original state before continuing.\n\u0026lt;\u0026lt; previous post: Untested Tests | next post: No Locality of Behavior \u0026gt;\u0026gt; ","permalink":"https://shaigeva.com/posts/10_footguns/03_the_tests_are_not_isolated/","summary":"\u003cp\u003e(this post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eWriting tests that are not isolated is a sure way to create unnecessary work for ourselves.\u003c/p\u003e\n\u003cp\u003eBy \u0026ldquo;tests that are not isolated\u0026rdquo;, I mean tests that sometimes have a different outcome (failing / passing) if we run\nonly a subset of them, if we run them in a different order or if we run them in parallel.\u003c/p\u003e\n\u003ch2 id=\"why-is-this-a-problem\"\u003eWhy is this a problem?\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s say we have 30 tests, and test 24 passes if we run it individually but fails if we run the entire test suite.\u003c/p\u003e","title":"Footgun #3 - The Tests Are Not Isolated"},{"content":"(this post is part of a series about good testing practices)\nSometimes our tests lie to us.\nWe have a test that was supposed to protect us from some bug, but that bug happened after all.\nOf course, what happened was that we made a mistake, and the test didn\u0026rsquo;t really verify what we thought it does.\nAs it turns out - when we write a test, it\u0026rsquo;s a good idea to spend a little effort to verify the test actually works.\nTo make sure that if the bug happens, the test does indeed fail.\nHow to avoid this My suggestion -\nWhen you write a test, for every assertion you write, make a small change:\nEither change the code a little and introduce the bug. Or change the test a little bit, so it verifies something a little different. This way you will be able to see if the test would have failed in the way you expect it to, and you\u0026rsquo;ll be able to count on it.\n\u0026lt;\u0026lt; previous post: There Are No Tests | next post: The Tests Are Not Isolated \u0026gt;\u0026gt; ","permalink":"https://shaigeva.com/posts/10_footguns/02_untested_tests/","summary":"\u003cp\u003e(this post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eSometimes our tests lie to us.\u003c/p\u003e\n\u003cp\u003eWe have a test that was supposed to protect us from some bug, but that bug happened after all.\u003c/p\u003e\n\u003cp\u003eOf course, what happened was that we made a mistake, and the test didn\u0026rsquo;t really verify what we thought it does.\u003c/p\u003e\n\u003cp\u003eAs it turns out - when we write a test, it\u0026rsquo;s a good idea to spend a little effort to verify the test actually works.\u003cbr\u003e\nTo make sure that if the bug happens, the test does indeed fail.\u003c/p\u003e","title":"Footgun #2 - Untested Tests"},{"content":" My name is Shai Geva.\nI\u0026rsquo;ve been creating software for humans, with humans, for over 20 years.\nI\u0026rsquo;m mostly a builder, but have also played more \u0026ldquo;high level roles\u0026rdquo;, like product and management (my current position is tech-lead at Sayata).\nI write about various topics, focusing on code quality and automated tests, which are a great passion for me.\nI also speak at confrences, so far exclusively about tests.\nIf you\u0026rsquo;d like to get notified or talk to me, please follow / ping me on twitter / x or linkedin.\n","permalink":"https://shaigeva.com/about/","summary":"\u003cdiv style=\"position: relative; display: inline-block;\"\u003e\n  \u003cimg src=\"/shai_mic_cover.jpg\" alt=\"Profile pic\" title=\"Profile pic\" style=\"margin:0; display: block; width: 100%;\" /\u003e\n  \u003cdiv style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; background: linear-gradient(to bottom, rgba(0, 0, 0, 0.1) 70%, rgba(0, 0, 0, 0.7) 100%); pointer-events: none;\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eMy name is Shai Geva.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve been creating software for humans, with humans, for over 20 years.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m mostly a builder, but have also played more \u0026ldquo;high level roles\u0026rdquo;, like product and management (my current position is\ntech-lead at \u003ca href=\"https://www.sayata.com/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eSayata\u003c/a\u003e).\u003c/p\u003e","title":"About"},{"content":"(this post is part of a series about good testing practices)\nThis is a \u0026ldquo;warm-up footgun\u0026rdquo; to the blog post series.\nThe easiest way to shoot yourself in the foot, testing-wise, is to have no tests at all.\nIn my experience, writing any tests often helps us - even if these tests are not well-written, and even if they\u0026rsquo;re just a drop in the sea.\nThere are a few reasons I noticed, why moving from no tests at all to even one test for some area of the code is useful.\nCode changes around hotspots Code changes are not uniformly distributed.\nIf we have a signifcantly-sized code base and we look 6 months to the past, we will see that changes tend to happen in the same places. We don\u0026rsquo;t change all types of features all the time and all types of infrastructure all the time.\nFeatures tend to evolve iteratively, and bugs that are introduced are statistically fixed in the following weeks or months.\nSo chances are - if you\u0026rsquo;ve changed an area of the code today, you will change it again in the next month.\nFor automated tests, what this means is that adding tests for the code we\u0026rsquo;re working on now tends to have a much larger impact than we might assume, because they are likely to protect us in the next few weeks.\nZero to one As with many, many considerations in any kind of project - making the first move is an uncomfortable \u0026ldquo;awkwardness\u0026rdquo; - we don\u0026rsquo;t exactly know how to do it.\nBut once we have a single test, we have an initial \u0026ldquo;paved road\u0026rdquo;, and we can keep improving in iterative steps, which is far easier.\nIf you never start, it\u0026rsquo;s 100% you\u0026rsquo;ll stay at zero Maybe the first test you\u0026rsquo;re writing will be the last.\nBut maybe it won\u0026rsquo;t, and it\u0026rsquo;ll lead to way to having a much better developer experience.\nIf you never write the first test, you will definitely not write tests number 2, 3 and 20.\nMy own experience I can\u0026rsquo;t back this up with anything besides \u0026ldquo;I\u0026rsquo;ve seen this enough, that\u0026rsquo;s the way it is\u0026rdquo; - but I can tell you that I almost never regretted writing the first few tests for a piece of code.\nIn almost all cases, it did help me improve the code design and prevent bugs.\nConclusion Start with something easy and simple, as long as you start :)\nnext post: Untested Tests \u0026gt;\u0026gt; ","permalink":"https://shaigeva.com/posts/10_footguns/01_there_are_no_tests/","summary":"\u003cp\u003e(this post is part of a \u003ca href=\"https://shaigeva.com/posts/10_footguns/ten_footguns/\"\u003eseries\u003c/a\u003e about good testing practices)\u003c/p\u003e\n\u003cp\u003eThis is a \u0026ldquo;warm-up footgun\u0026rdquo; to the blog post series.\u003c/p\u003e\n\u003cp\u003eThe easiest way to shoot yourself in the foot, testing-wise, is to have no tests at all.\u003c/p\u003e\n\u003cp\u003eIn my experience, writing any tests often helps us - even if these tests are not well-written, and even if they\u0026rsquo;re just a drop in the sea.\u003c/p\u003e\n\u003cp\u003eThere are a few reasons I noticed, why moving from no tests at all to even one test for some area of the code is useful.\u003c/p\u003e","title":"Footgun #1 - There Are No Tests"},{"content":"10 Ways To Shoot Yourself In The Foot With Tests - PyCon-IL 2024 (Hebrew) (English version below)\nThe talk shares hard-learned advice about how to avoid getting yourself in trouble with tests.\nSlides: Slideshare PyCon IL 2024\n10 Ways To Shoot Yourself In The Foot With Tests - PyCon-US 2023 (English) Given at Salt Lake City, Utah. The talk shares hard-learned advice about how to avoid getting yourself in trouble with tests.\nProperty-based testing - PyWeb-IL zoom meetup (Hebrew) General intro to property-based testing. Similar to the below talk but more python oriented, with more concrete examples. (and in zoom)\nProperty-based testing - Reversim2021 convention talk (Hebrew) General intro to property-based testing (not language specific)\n","permalink":"https://shaigeva.com/talks/","summary":"\u003ch1 id=\"10-ways-to-shoot-yourself-in-the-foot-with-tests---pycon-il-2024-hebrew\"\u003e10 Ways To Shoot Yourself In The Foot With Tests - PyCon-IL 2024 (Hebrew)\u003c/h1\u003e\n\u003cp\u003e(English version below)\u003c/p\u003e\n\u003cp\u003eThe talk shares hard-learned advice about how to avoid getting yourself in trouble with tests.\u003c/p\u003e\n\u003cp\u003eSlides: \u003ca href=\"https://bit.ly/testing_footguns_pycon_il_2024\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eSlideshare PyCon IL 2024\u003c/a\u003e\u003c/p\u003e\n\n\n    \n    \u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n      \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"allowfullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube.com/embed/k-vDmoPT84g?autoplay=0\u0026controls=1\u0026end=0\u0026loop=0\u0026mute=0\u0026start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\n      \u003e\u003c/iframe\u003e\n    \u003c/div\u003e\n\n\u003chr\u003e\n\u003ch1 id=\"10-ways-to-shoot-yourself-in-the-foot-with-tests---pycon-us-2023-english\"\u003e10 Ways To Shoot Yourself In The Foot With Tests - PyCon-US 2023 (English)\u003c/h1\u003e\n\u003cp\u003eGiven at Salt Lake City, Utah.\nThe talk shares hard-learned advice about how to avoid getting yourself in trouble with tests.\u003c/p\u003e","title":"Talks"}]